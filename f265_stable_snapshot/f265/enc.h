// Copyright (c) 2014, VANTRIX CORPORATION. All rights reserved. See LICENSE.txt
// for the full license text.


///////////////////////////////////////////////////////////////////////////////
// Documentation.
//
// This file defines bit depth dependent data structures and functions. It also
// describes the internal implementation.
//
//
// Supported features.
//
// A frame is split in slices, tiles and WPP rows. A slice is split in segments.
// A segment is split in chunks. A chunk corresponds to the escaped bytes
// generated by one run of the CABAC engine (initialization, encode, flush).
//
// There is an entry point at the start of every slice, tile and WPP row. An
// entry point causes a CABAC context reset.
//
// Segments comes in two types. An independent segment has a full header. A
// dependent segment has a partial header. Both types of segments may cover
// multiple entry points (tiles and WPP rows). When that happens, the
// specification imposes many restrictions, which usually require the segment to
// cover the entry points fully. A segment may also cover only a part of an
// entry point.
//
// In this implementation, the frame is split in sections that are independently
// processable by a thread. A section covers a contiguous range of CTBs in
// encoding order. Examples:
// - A section that covers a part of a tile:
//   - The section must be a slice because it must be independently processable.
//   - There may be one or several segments. The first segment is independent.
// - A section that covers two tiles fully:
//   - The segments can be of any type.
//   - There may be a segment that covers both tiles or several segments
//     that cover a part of a tile.
//
// We support tiles, slices, WPP and both types of segments.
//
// To simplify intra/inter prediction, we enforce the WPP restrictions on the
// slice layout (not segment layout) even when WPP is not used. Thus a slice can
// span multiple CTB rows only if it starts at the beginning of a CTB row.
//
// We support frame/slice/tile multithreading where each frame/slice/tile can be
// encoded concurrently by a thread. We do not support per-CTB synchronization.
// WPP rows are encoded in order, thus it is sufficient to keep one spare CABAC
// context per thread. WPP doesn't affect the supported multithreading modes.
//
// We support reencoding a frame, section or CTB. We do not support reencoding
// an individual segment or chunk.
//
// We support restrictions on the maximum segment size. In that case, a segment
// covers at most one entry point due to the inherent conflict with the
// specification restrictions.
//
// We support delayed SAO estimation. In this mode, we encode the CABAC
// bitstream in intermediate format so that the SAO parameters can be obtained
// after the frame has been fully deblocked. The segmentation occurs when the
// intermediate format is processed to inject the SAO parameters. All the usual
// features are supported in this mode.
//
// We do not support long-term reference frames or decodable leading frames
// because they have no practical use cases.
//
// We update the DPB after the current frame is retrieved from the lookahead and
// before the encoding data is allocated to the frame. This provides the
// opportunity to vacate reference frames using the latest statistics without
// requiring more memory. Since the memory of the vacated frames is freed, the
// DPB management is final once it is applied.
//
// We support both IDR and CRA refreshes (closed and open group of pictures).
// The frames displayed before the refresh frame are encoded before the frames
// displayed after the refresh frame. This is required by the specification and
// there are no use cases for a different order.
//
// We do not support the inter HV split. The specification forbids 4x4 inter
// partitions so the inter HV split cannot be used when 8x8 CBs are allowed. We
// do not care about the quality loss if the user intentionally selects low
// quality settings.
//
// We do not support disabling the deblocking/SAO filters on slice/tile
// boundaries since our implementation can efficiently filter the slice/tile
// boundaries.
//
//
// Frame bitstream encoding.
//
// The frame map specifies the layout of the sections, segments and tiles in a
// frame. The frame map can vary per frame, should we want to do so.
//
// Each thread has a buffer that contains the raw unescaped CABAC bitstream of
// the current CTB. For each frame encoded in parallel, there is a chunk buffer
// that contains the escaped chunks prior to NAL assembly. The frame output
// buffer contains the assembled frame NALs. The VPS/SPS/PPS and the slice
// headers are written during the NAL assembly.
//
// The chunk buffer is indexed by the index of the first CTB of a section in
// encoding order multiplied by the size allocated per CTB. At that location,
// the layout is as follow (32 bits per field).
//
//     Section: nb_segments S0 S1 ...
//     Segment 'S': flags ctb_address nb_chunks chunk0_size chunk1_size ...
//
// This is the minimal information we need to write the segment headers and
// assemble the NALs. For space reservation purposes, the worst case is 1
// segment and chunk per CTB.
//
// When we use delayed SAO estimation, the SAO buffer contains the intermediate
// bitstream before chunk encoding. The SAO buffer is indexed like the chunk
// buffer (max size per CTB). The SAO intermediate bitstream is a series of
// control and payload bytes, as follow.
//
//    Section: control payload0 payload1 ... control payload0 payload1 ...
//    Control bits: high 2 bits is the type, low 6 bits is the message.
//    Control type 0 message 0: CTB end.
//    Control type 0 message 1: SAO injection point.
//    Control type 1: context bins follow, message is number of bins.
//    Control type 2: bypass bins follow, message is number of bins.
//    Context bin byte: high 7 bits is the context value, low 1 bit is the bin.
//    Bypass bins: bypass bins are packed together in a big-endian byte string
//                 that is byte-aligned at the end. FIXME: consider using
//                 2-bits for nb_bits (1-4) and 4 bits for the bypass (1 byte).
//
// We allocate buffers for the worst case given the encoding parameters. Thus,
// the encoding is guaranteed to succeed and we need not check for buffer
// overflows. If we support PCM, we allocate smaller buffers since the encoder
// can fallback to 8x8 PCM CBs. Otherwise, we allocate larger buffers to encode
// all coefficients without PCM. Details follow.
//
// Per 16-bit coefficient, no PCM: 35 bypass bins.
// - With Rice=0: 1 sign bit, 4+15 prefix bits, 15 suffix bits.
//
// Per 4x4 TB in an image component: 34 context bins. A 8x8 TB consumes less
// context bins than 4 4x4 TBs because there are less gt1_flags.
// - 0.5 split_transform_flag (from 32x32 down to 4x4).
// - 1.5 CBF flags (from 32x32 down to 4x4).
// - 1 transform_skip_flag.
// - 0 coded_sub_block_flag (inferred for 4x4).
// - 16 sig_coeff_flags.
// - 8 gt1_flags.
// - 1 gt2_flag.
// - 2*3 last_sig_coeff prefix bins (value 111). There are no suffix bins.
//
// Per 4x4 PB: 23 context bins, 34 bypass bins.
// - We have at most one motion vector and reference index per 4x4 block.
// - merge_flag: 1 context bin.
// - inter_pred_idc: inferred.
// - ref_idx: 15 context bins.
// - mvp_l0_flag: 1 context bin.
// - MVD flags: 2*3 context bins.
// - MVD levels: 2*17 bypass bins.
//   - Maximum MVD: assuming 1024 qpel.
//   - With EG1: 1 sign bit, 8 prefix bits, 8 suffix bits.
//
// Per 8x8 CB: 11 context bins, 14 bypass bins:
// - split_cu_flag: 0.5 context bins (from 64x64 down to 8x8).
// - cu_transquant_bypass_flag: 1 context bin.
// - cu_skip_flag: 1 context bin.
// - pred_mode_flag: 1 context bin.
// - part_mode: 2 context bins.
// - cu_qp_delta_abs: 5 context bins, 14 bypass bins.
//   - 5 context bins for the prefix.
//   - Maximum QP value to encode as bypass: (52+6*(14-8))/2 - 5 = 39.
//   - With EG0: 1 sign bit, 7 prefix bins, 6 suffix bins.
//
// Per SAO CTB: 5 context bins, 126 bypass bins.
// - merge: 2 context bins.
// - type_idx: 3 context bins, 3 bypass bins.
// - offset: 3*4*9 bypass bins.
//   - We assume a practical limit of 7 on the offset.
//   - 1 sign bit.
// - band_position: 3*5 bypass bins.
//
// CABAC context encoding:
// - The entropy loss is maximal near the 50% probability. We assume 1.5 bit
//   per bin on the averaged worst case. This is probably overkill. The only way
//   to have a string of bad predictions is to have an even larger string of
//   good predictions.
// - An evil CTB can store bad probability values, requiring a locally larger
//   correction (up to 6 bits per bin) in the next CTB. On average, it doesn't
//   matter. However, for the per-CTB CABAC buffer, we assume that we need 10
//   extra bits per context to handle this local correction.
//
// CTB CABAC buffer size:
// - Object count: 1 SAO, 8*8 CBs, 16*16 PBs, 3*16*16 TBs, 3*64*64 coefficients.
// - Context bins: 1*5 + 8*8*11 + 16*16*23 + 3*16*16*34 = 32709.
// - Bypass bins: 1*126 + 8*8*14 + 16*16*34 + 3*64*64*35 = 439806.
// - Correction bits: F265_NB_CABAC_CTX*10.
//
// When escaping, the worst case is 3 bytes for every two RBSP bytes (e.g.
// 000003, 000003, ...).
//
//
// Frame padding and motion estimation/compensation bounds.
//
// A frame is padded by replicating the pixels on its boundaries. The padding is
// necessary to handle motion estimation and motion compensation efficiently.
//
// The motion compensation processes 64x64 reference blocks that lie far outside
// the frame. In that case, the motion vector is clipped to point to ~4 pixels
// outside the frame. The luma quarterpel filter refers to up to 4 pixels
// outside the block, depending on the block edge. Since the pixel on the
// boundary is replicated, the clipping ensures that 1+3=4 replicated pixels are
// present on each side of the block. The clipping does not change the result of
// the motion compensation. Since the pixels are replicated outside the edge,
// the quarterpel filtering occurring perpendicularly to the edge yields the
// same value regardless of the quarterpel component. The chroma quarterpel
// filter is 2-taps so we need not worry about it. The minimum frame padding is
// thus RoundEven(64+3+4)=72 pixels.
//
// The search boundaries are determined by several factors:
// - Frame padding.                     ----------------- <--- Padding boundary.
// - Frame multithreading.              | ------------- | <--- Search boundary.
// - Search algorithm.                  | | --------- | | <--- Frame boundary.
// - Motion vector cost table size.     | | |       | | |
//                                      | | | Frame | | |
//                                      | | |       | | |
//                                      | | --------- | |
//                                      | ------------- |
//                                      -----------------
//
// The cost of a motion vector component (|PMV-MV|) is computed with the motion
// vector cost table. This table is offsetted by the PMV so that indexing the
// table with the MV yields the cost directly.
//
// The motion search area is centered around the source block and is bounded
// both by the frame boundaries and the maximum motion vector size. The motion
// search range additionally limits the search around the predicted motion
// vector (PMV) in the search area. This design prevents motion vector drifts
// and cost table overflows.
//
// The PMV can grow arbitrarily large due to the scaling by the POC distance.
// The motion compensation can handle any PMV. However, because we clip the
// searched MV to the search area, the difference between the PMV and the
// searched MV can overflow. To handle this case, we make the cost table larger
// than the search area. A PMV outside of but close to the search area does not
// trigger an overflow. When the PMV is far outside the search area, the motion
// search is skipped for that PMV.
//
// The search bounds are also lowered by the out-of-bounds search tolerance. For
// performance, the search algorithm considers blocks that lie up to 6 pixels
// outside the search area without checking the bounds. This value is optimized
// for the Hexagon algorithm, which can search 3+1.5+0.75=5.25 pixels outside
// the area for fullpel/halfpel/quarterpel searches.
//
// When frame multithreading is used, the motion compensation area is restricted
// to the part of the reference frame that was encoded so far. PMVs that point
// in the not-yet-encoded area cannot be used for motion compensation and the
// skip and merge modes return MAX_COST in that case. Those PMVs can still be
// searched inside the search area.
//
//
// Blocks.
//
// The blocks can be scanned (visited) in several orders. The raster scan
// visits the blocks top-to-bottom, left-to-right. The depth scan visits the
// blocks as the leafs of a quadtree, in raster scan. The breadth scan visits
// the parent blocks of a quadtree before their children, in raster scan.
//
//      Raster scan        Depth scan
//        [0123]             [0145]  [wx] => recursively scan w before x, y, z.
//        [4567]             [2367]  [yz]
//        [89AB]             [89CD]
//        [CDEF]             [ABEF]
//
//                       Breadth scan
//     [1*64x64, 4*32x32, 16*16x16, 64*8x8, 256*4x4, ...]
//
// The working CTB contains information about each block inside the CTB and
// outside its border. For a 64x64 CTB, the block counts are as follow.
//   Blocks inside the CTB: 1*64x64, 4*32x32, 16*16x16, 64*8x8, 256*4x4.
//   Blocks inside and outside the CTB: 100*8x8, 324*4x4.
//   CBs inside the CTB: 1 + 4 + 16 + 64 = 85.
//
//                The 100 8x8 blocks layout
//        DBBBBBBBBC
//        A........E        Legend:
//        A........E          A are the blocks directly to the left (8).
//        A........E          B are the blocks directly to the top (8).
//        A........E          C is the block directly top-right (1).
//        A........E          D is the block directly top-left (1).
//        A........E          . are the blocks inside the CTB (64).
//        A........E          E are the unavailable blocks (18).
//        A........E
//        EEEEEEEEEE
//
//
// The block neighbours are named as follow.
//
//      D |   B   |  C
//     ---+-------+-------
//        |       |                Legend:
//      A |       |                  A: left.
//        |       |                  B: top.
//     ---+-------+                  C: top-right.
//        |                          D: top-left.
//      E |                          E: bottom-left.
//        |
//
// CTB invariants based on the slice layout restrictions:
// - E is always absent.
// - D is present if and only if both A and B are present.
// - C is absent if B is absent.
// - All the other combinations are possible.
//
//
// CTB analysis. Resume of future implementation ideas.
//
// When using the rate-distortion model (RDM), we estimate the costs of the
// syntax elements without encoding them, e.g. the transform tree cost is based
// on the prediction distortion.
//
// When doing rate-distortion optimization (RDO), we compute the precise cost of
// each syntax element. This trades a lot of performance for a lot of quality.
//
// * There are three logical passes at the CTB level. All passes are optional.
//
// We require that every pass set the CB data correctly, even if the pass is
// not the last one.
//
// ** Filter pass. Prune uninteresting CBs and modes. Decision logic:
// - Split cost is much below the unsplit cost: interest=split (unambiguous case).
// - Split cost is below the unsplit cost: interest=both (ambiguous case).
// - Split cost is greater than the split cost: interest=unsplit (unambiguous case).
//
// If the best decision is the unsplit case, then the split cost is expected to
// be close since the split prediction is usually just as good or better. The
// split overhead should bring the split cost slightly higher than the unsplit
// cost. We assume that the split case can be discarded safely when this happens.
//
// Several strategies can be used to explore the CB tree. The size of the
// ambiguity window can be adjusted. We can also consider the QP to fast-track
// some decisions.
//
// ** Layout pass. Determine the optimal CB layout.
//
// Visit the CBs bottom-up, according to the interest flags of the filter pass,
// if any. Update the interest flags. Keep the best CB layout.
//
// ** Refinement pass. Refine the CB modes in the optimal layout.
//
// Refine every CB in encoding order using the interest flags.
//
// * CB interest flags:
// - 2*16 reference indices (combined for all partition modes).
// - 35 intra modes (combined for all partition modes).
// - 2 intra part modes.
// - 9 inter part modes (UN, H1-3, V1-3, skip, empty slot for HV).
// - intra flag.
// - inter flag.
// - split flag.
//
// * QP discovery:
//
// Iteration on QPs (RDO_QG) occur at the CB or QG level (like HM).
//
// * Computing the residual distortion.
//
// When the coefficients are not encoded, the distortion is computed from the
// prediction. The distortion implicitly accounts for the coefficient bit
// costs. The lambda balances the CB header bits against the distortion.
//
// When the coefficients are encoded, the distortion is computed from the
// reconstruction. The lambda balances both the CB header bits and the
// coefficient bits against the distortion.
//
// * Computing the cost of a syntax element.
//
// ** Actual costs.
//
// The cost is computed from the binarization and the CABAC context values.
// This requires the CABAC contexts to be reset every time a new mode is
// tested. The running costs are accumulated in cbs->rdo_bits.
//
// To compare modes, we compute rdo_bits*lambda + dist.
//
// ** Estimated costs.
//
// The costs are precomputed in a table at the CTB or CB level. All possible
// syntax element values are computed (split flags, reference indices, MPM bits,
// etc.) from the current values of the CABAC contexts. This is a local estimate
// of the costs, which might be better than a static model. The precomputed
// costs are premultiplied by lambda to save time (vectorized multiplies).
//
// To compare modes, we compute bit_cost + dist. We do not encode coefficients
// when running in this mode since the performance/quality tradeoff seems bad
// (using the actual costs would only be marginally slower then).
//
// Updating the table at the CB level would require performing an RDO/actual
// encode of the previous CBs.
//
// * Rate distortion optimization level for intra prediction.
// - Level 0:
//   - Predict the neighbour pixels from the source pixels.
//   - Compute the distortion from the prediction.
//   - May early terminate the transform tree exploration.
// - Level 1:
//   - Predict the neighbour pixels from the reconstructed pixels.
//   - Compute the distortion from the reconstruction.
//   - Encode the coefficients.
// - Level 2:
//   - Like level 1, and do RDOQ.
//
// * Intra search strategy:
// - Strategy 0:
//   - Test planar, DC, H, V.
// - Strategy 1:
//   - Test planar, DC, H, V and the 3 diagonal modes.
// - Strategy 2:
//   - Like strategy 1, and use a dichotomic search to refine the angular
//     modes.
// - Strategy 3:
//   - Test the modes specified by the caller.
//
//
// Nomenclature for positions and offsets.
//
// Offsets are relative. Positions are absolute. We use offsets to locate a
// block within a CB or a CTB. We use positions to locate a block in the frame.
// For simplicity, we express offsets and positions in pixels. The offsets are
// stored in array format in data structures so we can access them in a loop.
//
// Nomenclature for variable names:
// (ct_ox,ct_oy): offsets of a block from the CTB origin.
// (cb_ox,cb_oy): offsets of a block from the CB origin.
// (px,py):       position of a block in a frame.
// "s"/"sub_":    prefix added to refer to a sub block within the current block.
//
//
// Block dimension packing.
//
// For the block-based functions, we dispatch according to the block width (the
// functions loop on the vertical component). The block widths are ordered in one
// of the two following schemes:
//   Square: 4, 8, 16, 32.
//   AMP:    2, 4, 8, 16, 32, 64, 6, 12, 24, 48.
//
// The index for the square/AMP case is called the square/AMP width index
// (swi/awi).
//
// In the square case, the index is lg_bs - 2. For instance, block 8x8 maps to
// index 1.
//
// In the AMP case, the index is based on the lg_bs of the square block on which
// AMP is applied. Given a square block of size 1<<lg_bs, we obtain the index
// for each split fraction as follow.
//   4/4: lg_bs - 1. For instance, block 8x8 maps to index 2.
//   3/4: lg_bs + 3. For instance, block 6x8 maps to index 6.
//   2/4: lg_bs - 2. For instance, block 4x8 maps to index 1.
//   1/4: lg_bs - 3. For instance, block 2x8 maps to index 0.
//
// Functions operating on AMP block sizes receive the block dimensions in a
// packed integer:
//      packed_dims = awi<<24 | (bd-8)<<16 | width<<8 | height
//
// This packing reduces the number of arguments passed in function calls. The
// assembly code typically ignores the width since the dispatching is based on
// the block width. The width is still available for use in the C code. The
// height is packed first so it can be accessed quickly in assembly by masking
// with 0xff. The bit depth is passed as bd-8 so that setting up the bit depth
// is a no-op at low bit depth. Since the bit depth is always present, the API
// is identical at low and high bit depths.


///////////////////////////////////////////////////////////////////////////////
// Includes.

#ifndef F265_ENC_H
#define F265_ENC_H

#include "f265/bdi.h"

// The preprocessor symbol F265_LBD/HBD is defined if this file is being
// compiled for low/high bit depth.

// The string "venc" gets replaced by "f265_lbd" or "f265_hbd" depending on the
// bit depth. "venc" is used in function names to avoid symbol clashes. It is
// not used in data structure names. The substitution is done before the file is
// parsed by the C preprocessor.
//
// f265_pix/F265_PSIZE are replaced by the pixel size, unsigned.
// f265_pix2/F265_PSIZE2 are replaced by twice the pixel size, signed.
//
// FIXME: PSIZE2 is probably useless. Consider recompiling the pure C code with
// Haswell support to benefit from inline assembly and the BMI2 instructions.
// Dispatch from the bdi entry point.
#ifdef F265_LBD

#define f265_pix uint8_t
#define f265_pix2 int16_t
#define F265_PSIZE 1
#define F265_PSIZE2 2

#else

// Temporarily replacing the pixel type for HM ME compatibility. The extra bit
// is not needed since the maximum bit depth is 14.
#ifdef VAN_USE_HM_ME
#define f265_pix int16_t
#else
#define f265_pix uint16_t
#endif

#define f265_pix2 int32_t
#define F265_PSIZE 2
#define F265_PSIZE2 4

#endif

// FIXME: figure out the worst case.
#define F265_STASH_BUF_SIZE         (1024*1024)
#define F265_STORE_BUF_SIZE         (1024*1024)

// CTB CABAC buffer size in the worst case.
#define F265_CABAC_BUF_SIZE         ((F265_NB_CABAC_CTX*10 + 32709*3/2 + 439806)/8)


///////////////////////////////////////////////////////////////////////////////
// Data structures.

struct f265_me_ctx; typedef struct f265_me_ctx f265_me_ctx;
struct f265_ref_ctx; typedef struct f265_ref_ctx f265_ref_ctx;
struct f265_cabac_bs; typedef struct f265_cabac_bs f265_cabac_bs;
struct f265_frame; typedef struct f265_frame f265_frame;
struct f265_enc_thread; typedef struct f265_enc_thread f265_enc_thread;

// Node of a doubly-linked list.
typedef struct f265_list_node
{
    // Pointer to the next node.
    struct f265_list_node *next;

    // Pointer to the previous node.
    struct f265_list_node *prev;

    // Pointer to the data contained in this node.
    void *data;

} f265_list_node;

// Motion vector.
typedef union f265_mv
{
    int16_t v[2];                       // Vector values.
    struct { int16_t x; int16_t y; };   // Named values.
    uint32_t p;                         // Packed values.

} f265_mv;

// Weighted prediction weight.
typedef struct f265_weight
{
    // True if wp is used.
    int8_t used_flag;

    // Log2 value of the denominator to allow fast division by shifting.
    int16_t log2_denom;

    // Numerator value.
    int16_t num;

    // Offset value.
    int16_t off;

} f265_weight;

// HM compatibility GOP entry.
typedef struct f265_hm_gop_entry
{
    // Frame type.
    int type;

    // POC offset of the current frame in the current GOP.
    int poc_off;

    // QP offset.
    int qp_offset;

    // QP factor.
    double qp_factor;

    // Maximum number of frames used as reference in each list.
    int nb_active;

    // True if the frame is referenced by another frame.
    int ref_flag;

    // Number of frames kept as reference.
    int nb_refs;

    // POC offsets from the current frame of the frames kept as reference.
    int refs[16];

} f265_hm_gop_entry;

// Function type declarations.
typedef void (*f265_block_copy_func)(f265_pix *dst, int32_t dst_stride, f265_pix *src, int32_t src_stride);
typedef void (*f265_weight_func)(f265_pix *dst, int32_t dst_stride, f265_pix *ref, int32_t ref_stride,
                                  f265_weight *weight);
typedef void (*f265_inter_luma_func)(f265_pix *dst, int32_t dst_stride, int32_t src_stride, f265_pix *src0,
                                     f265_pix *src1);
typedef void (*f265_inter_chroma_func)(f265_pix *dst, int32_t dst_stride, f265_pix *src, int32_t src_stride, int32_t mx,
                                       int32_t my);
typedef int32_t (*f265_dist_func)(f265_pix *src0, int32_t stride0, f265_pix *src1, int32_t stride1,
                                  int32_t width, int32_t height, int32_t bitdepth);
typedef void (*f265_multi_dist_func)(f265_pix *src, f265_pix **ref_array, int32_t ref_stride, uint32_t *cost_array,
                                     f265_me_ctx *me);
typedef f265_pix* (*f265_ref_func)(int32_t mx, int32_t my, f265_me_ctx *me);
typedef int32_t (*f265_clip_cand_func)(f265_mv out_array[12], f265_mv in_array[12], int32_t nb_mv, int32_t dup_off,
                                       uint64_t mv_bounds64);
typedef void (*f265_me_search_func)(int32_t nb_iter, int32_t scale, f265_me_ctx *me);

// Set of pixel functions used to operate on a specific block size.
typedef struct f265_block_funcs
{
    // Block size identifier. In order:
    // UN, H2, V2, H1, H3, V1, V3 (64x64, 32x32, 16x16).
    // UN, H2, V2, HV (8x8).
    int32_t bs;

    // Width and height of the block in terms of pixels.
    int32_t dim[2];

    // Function pointers.
    f265_block_copy_func copy;
    f265_weight_func weight;
    f265_inter_luma_func inter_luma;
    f265_inter_chroma_func inter_chroma;

    // The index is 2*metric_id + chroma_flag. The non-chroma version tests only
    // the luma component.
    f265_dist_func dist[4];
    f265_multi_dist_func dist3[4];
    f265_multi_dist_func dist4[4];

} f265_block_funcs;

// Data used to do the quarterpel interpolation of a reference block (temporary
// interface).
typedef struct f265_frac_ref_block
{
    // True if bi-prediction.
    int bipred_flag;

    // Reference context for single/bi-prediction (L1 appears at index 0 if
    // there is no bi-prediction).
    f265_ref_ctx *ctx[2];

    // Motion vectors, same layout as above.
    f265_mv mv[2];

    // Position in the plane (XY).
    int pos[2];

    // Size of the block (XY).
    int size[2];

    // Image component (YUV).
    int comp;

} f265_frac_ref_block;

// Structure that keeps the neighbour's MV information.
typedef struct f265_inter_neighbour_mv
{
    // Neighbour MV. The index represents the list.
    f265_mv mv[2];

    // In the case of spacial prediction, this field contains the reference
    // index in each reference list (-1 if not available).
    //
    // In the case of temporal prediction, the reference index values are
    // precomputed and interpreted differently. The first value is used for the
    // prediction with list 0 (i.e. for a PMV in list 0). The second value is
    // used for the prediction with list 1. For each value, the first four bits
    // are the reference index associated to the PMV, and the fifth bit is the
    // reference list associated to the PMV. See usage.
    //
    // Note: if both reference indices are -1, then unified_ref is also -1.
    union { int16_t ref_idx[2]; int32_t unified_ref; };

} f265_inter_neighbour_mv;

// Data about the current inter block.
typedef struct f265_inter_block
{
    // Neighbour MVs of the current partition. 5 spatial candidates and the
    // temporal candidate.
    f265_inter_neighbour_mv neighbour_mv[6];

    // Temporary data used for predicting the mode of the current partition. See
    // usage. FIXME, probably not as useful as first envisaged.
    f265_mv part_mv[2];
    int8_t part_pmv_idx[2];
    int8_t part_ref_idx[2];

    // PMVs of the best reference index in each reference list in the current
    // partition. We cache the PMVs to speed up the selection of the best PMVs
    // after the bi-prediction refinement.
    f265_mv uni_pmv[2][2];
    int8_t uni_pmv_idx[2];

    // PMVs of the current CB inter mode. We cache the PMVs to speed up the MVD
    // cost computation (TODO).
    f265_mv mode_pmv[2][2];

    // Best prediction data associated to the current CB. The inter mode is set
    // to -1 to identify the skip case, otherwise it is the partitioning mode.
    int64_t cb_best_cost;
    int cb_inter_mode;
    f265_mv cb_mv[2][2];
    int8_t cb_ref_idx[2][2];
    uint8_t cb_inter_bf[2];

} f265_inter_block;

// Data about the analysis.
// FIXME rename to f265_analysis?
typedef struct f265_an_block
{
    // Intra search strategy.
    int8_t intra_search_strategy;

    // We may override the encoder parameters with local parameters during
    // analysis to speed up the analysis.
    uint8_t tb_range[2];
    uint8_t tb_depth[2];

    // Mode comparison metric.
    int8_t mode_metric;

    // Consider the chroma component during the intra/inter search.
    int8_t chroma_me_flag;

    // True if estimated costs are used for syntax elements.
    int8_t rdm_flag;

    // Use HM-compatible motion estimation.
    int8_t hm_me_flag;

    // Test all intra modes.
    int8_t all_intra_flag;

    // Nullify the inter transform blocks.
    int8_t nullify_inter_tb_flag;

    // Rate-distortion optimization lambda in 1/256 fractions.
    int rdo_lambda;

    // Rate-distortion model lambda in 1/256 fractions.
    int rdm_lambda;

    // Approximate syntax element costs. The costs are premultiplied by the
    // lambda for performance. Eventually we might want to save the raw costs
    // to update this table quickly when the QP changes.
    uint16_t se_costs[F265_SE_SIZE];

    // Kludge to compute the distortion prior to clamping the samples.
    // The HM works in the residual domain during the inter RDO analysis.
    int compute_ssd_from_idct2;
    int idct2_dist;

} f265_an_block;

// Early motion estimation parameters.
typedef struct f265_early_me_params
{
    // Distortion function ID used for fullpel/halfpel/quarterpel. A distortion
    // function ID is 2*metric_id + chroma_flag.
    int8_t dist_func_ids[3];

    // Number of iterations of the search algorithm functions.
    int16_t nb_iters[3];

    // Block reference function for subpel motion estimation.
    f265_ref_func ref_func;

    // Motion vector candidate clipping function.
    f265_clip_cand_func clip_cand_func;

    // Search algorithm function for fullpel/halfpel/quarterpel.
    f265_me_search_func search_funcs[3];

} f265_early_me_params;

// Contains all information relative to the reference.
typedef struct f265_me_ref
{
    // Predicted motion vector (quarterpel).
    f265_mv pmv;

    // Best motion vector (quarterpel).
    f265_mv best_mv;

    #ifdef VAN_USE_HM_ME
    // FIXME. Change hm_best_mv to best_mv after the merge.
    f265_mv hm_best_mv;
    #endif
    // Pointer to the weighted prediction weights.
    f265_weight *weights;

    // Motion vector cost table indexed with each component of the predicted
    // motion vector.
    uint16_t *mv_costs[2];

    // Block reference planes (eventually should be the same as f265_ref_ctx).
    // Currently using 0,1,2 as YUV. FIXME: scheduled for removal.
    f265_pix *ref_planes[3];

    // Reference planes, in this order: Y YHH YHV YHD U V W.
    f265_pix *planes[7];

} f265_me_ref;

// Motion estimation context. FIXME. The original H.264 design made that object
// fully independent from the thread. Ultimately this was a bad design decision.
// Remove the data that belongs to the thread from this object eventually.
struct f265_me_ctx
{
    // Best cost of the block (distortion + motion vector cost).
    int best_cost;

    // Block plane offset, for luma and chroma.
    int plane_off[2];

    // Packed block dimensions, for luma and chroma.
    int packed_dims[2];

    // Chroma scale factor, horizontal and vertical.
    int csf[2];

    // True if chroma motion estimation is used.
    int8_t chroma_flag;

    // Distortion function ID.
    int8_t dist_func_id;

    // Lambda in 1/256 fractions.
    int lambda;

    // Pointer to the early motion estimation parameter array, if any.
    f265_early_me_params *early_me_params;

    // Motion vector cost table indexed with the zero MV offset.
    uint16_t *mv_cost_table;

    // Motion estimation bounds (X min, Y min, X max, Y max).
    union { int16_t me_bounds[4]; uint64_t me_bounds64; };

    // Packed motion vector bounds (quarterpel). See f265_mv_out_of_range().
    uint32_t me_bounds_packs[2];

    // Block source luma and chroma planes.
    f265_pix *src_planes[3];


    // Those fields are scheduled for deletion.
    #if 1
    f265_enc *enc;
    f265_enc_thread *t;

    // Reference plane stride, in pixels.
    int32_t ref_stride;

    // Block position in the source plane, horizontal and vertical.
    int16_t pos[2];

    // Block dimension, horizontal and vertical.
    int16_t dim[2];

    // Information for each reference.
    // The second cell is used only on bi-prediction.
    //
    // Merge directly in this object, for one reference only.
    f265_me_ref ref[2];

    // Luma and chroma bit depth.
    int bit_depth[2];

    // SAD and SATD function pointers.
    f265_dist_func dist[2];
    #endif


    // These variables are needed for HM-compatible ME.
    #ifdef VAN_USE_HM_ME

    // Dual reference frame.
    f265_pix *hm_refd;

    // Preprocessed source.
    f265_pix *hm_new_src;

    // Sub-sampling shift.
    int32_t hm_subshift;

    // Source stride.
    int32_t hm_src_stride;

    // Search range. 0: uni-dir, 1: bi-dir.
    int16_t hm_range[2];

    // Search algo: 0:Full, 1:TZ.
    int8_t hm_algo;

    // ME lambda.
    int32_t hm_lambda_me;

    // TZ algorithm variables.

    // Best distance.
    int16_t hm_best_dist;

    // Best round.
    int16_t hm_best_round;

    // Best direction.
    int16_t hm_best_dir;
    #endif
};

// Information about a CTB in a frame.
typedef struct f265_frame_ctb
{
    // Quarterpel motion vector bounds (X min, Y min, X max, Y max) for motion
    // estimation. See the discussion on frame padding for details.
    union { int16_t me_bounds[4]; uint64_t me_bounds64; };

    // Same as above for motion compensation. This is used to clip predicted
    // motion vectors (e.g. for skip) that may point outside the frame padding.
    union { int16_t mc_bounds[4]; uint64_t mc_bounds64; };

    // (X,Y) position of the CTB in raster scan.
    int16_t pos[2];

    // CTB neighbour availability flags.
    uint8_t neighbours;

    // Segmentation flags.
    uint8_t seg_flags;

} f265_frame_ctb;

// Information about the division of a frame in sections, segments and chunks.
typedef struct f265_frame_map
{
    // Information about the sections. The first index is the section index. The
    // first value is the index of the first CTB of the section in encoding
    // order. The second value is the number of CTBs in the section.
    int (*section_map)[2];

    // Information about the CTBs in the frame. The index is the CTB index.
    f265_frame_ctb *ctb_map;

    // Number of sections.
    int nb_sections;

} f265_frame_map;

// Information about a segment chunk being encoded.
typedef struct f265_seg_chunk
{
    // Pointer to the beginning of the chunk.
    uint8_t *base;

    // Pointer to the write position in the chunk.
    uint8_t *cur;

    // Number of trailing zero bytes in the previous CTB of the chunk, tracked
    // for escaping.
    int trail_zeroes;

    // True if the current CTB is the first of the chunk.
    int start_flag;

} f265_seg_chunk;

// Object used to write the NAL bytestream.
typedef struct f265_nal_bs
{
    // Position where the NAL unit starts in the bytestream.
    uint8_t *nal_start;

    // Position where the NAL payload starts in the bytestream.
    uint8_t *data_start;

    // NAL format information.
    int format;
    int nal_size_len;

} f265_nal_bs;

// Object used to write and escape the VLC bitstream.
typedef struct f265_vlc_bs
{
    // Pointer to the first byte of the buffer. The buffer is assumed to be
    // large enough to write all the data.
    uint8_t *base;

    // Pointer to the current byte of the buffer. The data is written in byte
    // chunks.
    uint8_t *cur;

    // Number of bits required to complete the current byte.
    int bits_left;

    // Number of trailing zero bytes, tracked for escaping.
    int trail_zeroes;

} f265_vlc_bs;

// Raw CABAC data.
typedef struct f265_cabac_raw
{
    // Pointer to the first byte of the buffer. The buffer is assumed to be
    // large enough to write all the data. The data is written in byte chunks.
    // At most 4 bytes previously written can be modified when encoding the next
    // bin. The base is guaranteed to be 32-bit aligned.
    uint8_t *base;

    // Pointer to the current byte of the buffer.
    uint8_t *cur;

    // Division interval low and range (high = low + range).
    uint64_t low;
    uint32_t range;

    // Number of bits that can still be added to the low register before a
    // 32-bit chunk is output. Can become negative. See the implementation
    // details.
    int32_t bits_left;

    // Number of 32-bit (0xffffffff or 0x00000000) chunks awaiting to be written
    // in the bitstream.
    int32_t chunks_outstanding;

} f265_cabac_raw;

// SAO CABAC data.
typedef struct f265_cabac_sao
{
} f265_cabac_sao;

// RDO CABAC data.
typedef struct f265_cabac_rdo
{
    // Measure the entropy loss as fractions of bits with the denominator 32768.
    int64_t bits;

} f265_cabac_rdo;

// Object used to write the unescaped CABAC bitstream.
struct f265_cabac_bs
{
    // Mode-specific data.
    union
    {
        f265_cabac_raw raw;
        f265_cabac_sao sao;
        f265_cabac_rdo rdo;
    };

    // Pointers to mode-specific functions.
    void (*encode_context_bin)(f265_cabac_bs *cbs, int ctx_idx, int bin);
    void (*encode_bypass_bin)(f265_cabac_bs *cbs, int bin);
    void (*encode_bypass_bins)(f265_cabac_bs *cbs, int bins, int nb_bins);
    void (*encode_term0_bin)(f265_cabac_bs *cbs);

    // Encoding mode.
    uint8_t mode;

    // CABAC context array.
    uint8_t contexts[F265_NB_CABAC_CTX];
};

// Bitstream snapshot object for CABAC.
typedef struct f265_cabac_snapshot
{
    // Engine state. We include the last four bytes since they may still change.
    uint8_t *cur;
    uint64_t low;
    uint32_t range;
    int32_t bits_left;
    int32_t chunks_outstanding;
    uint32_t prev_chunk;

    // Context state.
    uint8_t contexts[F265_NB_CABAC_CTX];

} f265_cabac_snapshot;

// Context for a reference list index.
struct f265_ref_ctx
{
    // Reference frame.
    f265_frame *frame;

    // Reference planes, in this order: Y YHH YHV YHD U V W.
    f265_pix *planes[7];

    // Pointer to the weighted prediction weights.
    f265_weight *weights;
};

// SAO parameters of a CTB.
typedef struct f265_ctb_sao
{
    // Filter mode.
    uint8_t mode;

    // In edge mode, the direction. In band mode, the band offset.
    union
    {
        uint8_t edge_dir;
        uint8_t band_off;
    };

    // Pixel offsets.
    int8_t pixel_off[4];

} f265_ctb_sao;

// Transform subblock (4x4) encoding data. The data is packed in encoding order
// (last-to-first with zig-zag) unless stated otherwise.
typedef struct f265_sb_enc
{
    // Coefficient non-zero flags.
    uint16_t nz_flags;

    // Coefficient signs (0 is positive). Stored first-to-last so that bins can
    // be written directly.
    uint16_t signs;

    // Coefficient remaining flags (true if a coefficient remaining level must
    // be encoded).
    uint16_t remain_flags;

    // Coefficient greater-than-1 flags, packed.
    uint8_t gt1_flags;

    // Packed data.
    // 0-4: number of non-zero coefficients.
    // 5:   value of the greater-than-2-flag, if present.
    uint8_t packed_data;

} f265_sb_enc;

// Transform block encoding data.
typedef struct f265_tb_enc
{
    // Log of the transform block size.
    uint8_t lg_bs;

    // Scan order (up-right, horizontal, vertical).
    uint8_t order;

    // Subblock non-zero flags in encoding order, followed by the values of the
    // subblock flag of the neighbour to the right/below in raster scan.
    uint64_t nz_flags[3];

    // Subblock sign-hiding flags in raster scan.
    uint64_t sign_hiding_flags;

} f265_tb_enc;

// Pointers in the transform tree encoding data: current transform node,
// transform block, transform subblock and transform coefficient level.
typedef struct f265_tt_enc
{
    uint8_t *tn;
    f265_tb_enc *tb;
    f265_sb_enc *sb;
    int16_t *levels;

} f265_tt_enc;

// Parameters for venc_rec_block(). This is a stub.
typedef struct f265_rec_params
{
    // Pointer to the thread.
    f265_enc_thread *t;

    // Source buffer.
    f265_pix *src;
    int src_stride;

    // Prediction buffer.
    f265_pix *pred;
    int pred_stride;

    // Reconstruction buffer. Can be the same as the prediction buffer or null
    // to skip the reconstruction.
    f265_pix *rec;
    int rec_stride;

    // Transform tree data. Updated if non-null.
    f265_tt_enc *tt;

    // Quantization/dequantization scaling lists for the transform block. NULL
    // if not used.
    int16_t *list[2];

    // CABAC context array.
    uint8_t *cabac_contexts;

    // Quantization parameters.
    int quant_mult;
    int quant_add;
    int quant_shift;

    // Used for RDOQ (HM compatibility).
    double quant_temp;

    // Lambda value for RDOQ computation (HM compatibility).
    double lambda;

    // Image component.
    uint8_t comp;

    // Log of the transform block size.
    uint8_t lg_bs;

    // Transform block depth.
    uint8_t tb_depth;

    // True for DST, false for DCT.
    uint8_t dst_flag;

    // Scan order (up-right, horizontal, vertical).
    uint8_t order;

    // Force this block to be zero.
    uint8_t zero_flag;

    // True to use sign data hiding.
    uint8_t sign_hiding_flag;

    // True to use RDOQ.
    uint8_t rdoq_flag;

    // QP.
    uint8_t qp;

    // Bit depth.
    uint8_t bd;

    // True if this is an iframe.
    uint8_t iframe_flag;

    // True if this CB is intra.
    uint8_t intra_flag;

    // True to dump the transform steps (debug).
    uint8_t dump_flag;

    // FIXME: need flags for transform skip, transquant bypass.

    // Kludge to compute the distortion prior to clamping the samples.
    // The HM works in the residual domain during the inter RDO analysis.
    int compute_ssd_from_idct2;
    int idct2_dist;

} f265_rec_params;

// Data associated to a frame. The frame object is lightweight. Other objects
// are attached and detached dynamically from the frame object.
struct f265_frame
{
    // Temporary import from legacy lookahead data structure. FIXME.

    // Frame type assigned by the lookahead.
    int8_t la_frame_type;

    // GOP entry associated to the frame.
    f265_hm_gop_entry *gop_entry;


    // The following fields are invariant once they are set.

    // Pointers to the source YUV planes. All the planes have the same stride.
    // The chroma U and V planes are laid out contiguously in 4:2:0. The index
    // is the plane ID.
    //     [ Y Y Y Y Y Y Y Y ]
    //     [ Y Y Y Y Y Y Y Y ]
    //     [ U U U U V V V V ]
    //     [ U U U U V V V V ]
    //     [stride/2 stride/2]
    f265_pix *src_planes[3];

    // Pointers to the reconstructed luma halfpel planes + chroma planes.
    //     [ F H F H ]   F: fullpel.
    //     [ V D V D ]   H: horizontal halfpel.
    //     [ F H F H ]   V: vertical halfpel.
    //     [ V D V D ]   D: diagonal halfpel.
    f265_pix *rec_planes[4+2];

    // Pointers to the subsampled halfpel planes. The planes have half the
    // resolution of the fullpel plane.
    f265_pix *sub_planes[4];

    // Frame map.
    f265_frame_map *fmap;

    // SAO parameter cache. The index is the CTB number in raster scan.
    f265_ctb_sao *sao;

    // QP map. The index is the 8x8 block number in raster scan.
    int8_t *qmap;

    // 8x8 block map. The index is the 8x8 block number in raster scan. The
    // value is a bitfield:
    //   0: intra/inter.
    // 1-2: coding block size.
    // 3-4: horizontal/vertical deblocking edges (top/left).
    // 5-6: horizontal/vertical deblocking TB edges (top/left).
    //   7: transform block unfiltered flag.
    //   8: CB skip flag.
    uint16_t *b8_map;

    // 4x4 block map. The index is the 4x4 block number in raster scan. The
    // value is a bitfield:
    // 0-5: intra luma mode. Uninitialized if inter.
    //   6: luma transform block non-zero flag.
    uint8_t *b4_map;

    // Reference index and motion vector cache. The first index is the 4x4 block
    // number in raster scan. The second index is the reference list.
    // The reference index is initialized to -1 if the MV is invalid.
    int8_t (*ref_idx)[2];
    f265_mv (*mv)[2];

    // Frame timestamp and duration, in nano seconds.
    int64_t timestamp;
    int64_t duration;

    // Absolute picture order count. No reset on IDR. This field is set when the
    // frame data is received.
    int64_t abs_poc;

    // Absolute picture order count of the last IDR frame. 0 initially.
    int64_t last_idr_abs_poc;

    // H.265 picture order count. Reset on IDR frame. This is the real value,
    // not the wrap-around value.
    int32_t h265_poc;

    // Frame flags visible from all threads.
    uint32_t gen_flags;

    // Frame type.
    int8_t frame_type;

    // Slice header quantization parameter.
    int8_t qp;

    // Number of reference lists and reference indices in each list.
    int8_t nb_lists;
    int8_t nb_ref_idx[2];

    // Temporal reference frame information. The first 4 bits are the reference
    // index of the collocated frame (inside the reference list of the current
    // frame), the fifth is the reference list of the collocated frame, the
    // sixth determines if we force the candidate list on bi-prediction (i.e.
    // choose the list collocated_from_l0_flag).
    int8_t temp_ref_info;

    // Number of negative/positive frames (those displayed before/after the
    // current frame) in the DPB.
    int8_t dpb_neg, dpb_pos;

    // Total number of frames in the DPB.
    int8_t dpb_size;

    // DPB sorted by increasing POC. The current frame is not in the DPB.
    f265_frame *dpb[16];

    // POC of the reference frames for temporal prediction. The first index is
    // the reference list. The second index is the reference index.
    int64_t ref_poc[2][16];

    // Lookahead data. The data is exclusively set by the lookahead. The main
    // thread and the lookahead do not access this data concurrently. FIXME.
    //f265_la_frame la;


    // Data exclusive to the main thread.

    // Pointers to the buffers owning the dynamic objects. The pointers are
    // null when the objects are detached.
    // 0 => frame object.
    // 1 => source planes.
    // 2 => lookahead data.
    // 3 => encoding data.
    F265_ALIGN64 uint8_t *mem_buf[4];

    // Frame flags visible from the main thread.
    uint32_t main_flags;

    // Estimated cost.
    float est_cost;

    // Average QS of the frame. This is an estimation before encoding and the
    // actual value after encoding.
    float qs;

    // Number of qbits in the frame. Same remark as above.
    int64_t qbits;

    // Number of bits in the frame. Same remark as above.
    int32_t actual_bits;

    // Expected number of bits for the duration of the frame.
    int32_t expected_bits;
};

// Coding block data. Most intra and inter fields do not overlap so that we can
// set either information independently. The flag F265_CB_INTRA determines the
// actual CB mode. The CB flags must be consistent with the CB mode.
typedef struct f265_cb
{
    // Motion vectors. The first index is the partition number. The second is
    // the reference list.
    f265_mv mv[2][2];

    // Reference indices. Same indices as above. The reference index is -1 if
    // the list is unused.
    int8_t ref_idx[2][2];

    // Intra luma modes. The first luma mode is set to 35 for PCM. The second
    // luma mode is set to -1 if the partition mode is UN. FIXME: consider
    // exposing PCM as a CB flag.
    int8_t intra_luma_mode[4];

    // Intra chroma mode.
    int8_t intra_chroma_mode;

    // Inter partition mode (UN, H2, V2, H1, H3, V1, V3).
    int8_t inter_part;

    // Inter partition bitfields. The index is the partition number.
    // 0-2: merge mode (0-4: merge index, 5: not merged).
    // 3-4: PMV index for list 0/1.
    uint8_t inter_bf[2];

    // Desired luma QP. The actual QP may differ.
    int8_t qp;

    // Log of the coding block size.
    int8_t lg_bs;

    // Index of the first child CB in breadth scan.
    int8_t child_idx;

    // Encoding index (i.e. depth scan) of this CB in terms of 8x8 blocks. For
    // a CB located outside the current CTB, the index is -1 if the CB is
    // available, otherwise it is 127.
    int enc_idx;

    // Offset of the CB in the X, Y direction in terms of pixels from the CTB
    // origin.
    int8_t cb_off[2];

    // CB flags.
    uint8_t flags;


    #ifdef VAN_LOAD_HM_ME
    // These variables are needed to import the HM data for comparison and validation purposes.

    // List of MVP. first index: partition, second: ref list, third ref index, fourth mvp index.
    f265_mv hm_mvp_list[2][2][33][2];

    // ME lambda.
    int hm_lambda_me;

    // MVD L1 zero flag
    int hm_mvd_l1_zero_flag;

    // CB encoding mode (intra, inter, merge, ...)
    int hm_cb_mode;

    // Inter prediction mode bits and number of reference frames.
    // First index partition, second reference list.
    int hm_pmode_bits[2][2];
    int hm_num_of_ref[2][2];

    // Index is the partition.
    int hm_ref_list[2];    // L0, L1, or Bi

    // Final cost and bits for each inter prediction mode.
    // First index: partition, second: prediction mode.
    int hm_cost[2][3];
    int hm_bits[2][3];
    int hm_ref_idx[2][3];
    #endif

} f265_cb;

// The stash handles rolling back to the state of a block before and after the
// analysis.
//
// The intended usage is as follow. Before the analysis, we take a snapshot of
// the block. This is the initial state. Then, we encode the block with a given
// mode and we snapshot the resulting state. This is the saved state. We
// rollback to the initial state and try the next mode. If that mode is better
// than the saved mode, we clobber the saved state. We rollback to the initial
// state and continue testing modes. When the last mode has been tested, we
// update the current state to reflect the best state. If the best mode is the
// last tested mode, then the current state is already the best state (and
// updating the saved state is not required). Otherwise, we clobber the current
// state with the saved state.
//
// The stash operates as a stack. Pushing a stash frame saves the information
// of the current block and allocates memory for analysing another block
// (typically a child block). Popping the stash frame restores the previously
// saved information. Pushing a stash frame is necessary when both the parent
// state and the child state need to be saved.
//
// After a block has been analyzed, the current state is the best state and the
// current stash frame can be clobbered/popped safely. Exploring depth-first
// results in smaller stash frames. If there is only one mode for a block, then
// the stash needs not be used at all.
//
// The layout of the stash is as follow. The stash is not aligned.
//   --- State of the previous stash frame ---
//   stash_off[3], px, py, width, height, tn_off (see the fields below).
//   --- Initial state (stash_off[0]) ---
//   CABAC contexts.
//   --- Saved state (stash_off[1]) ---
//   Number of transform nodes saved.
//   Transform nodes.
//   Reconstructed pixels.
//   CABAC contexts.
//   --- Stash top (stash_off[2]) ---
//
// By design, the stash may make redundant copies of the CABAC contexts. For
// example, the first child block might share the same CABAC contexts as its
// parent. Nevertheless, the implementation is optimal because the housekeeping
// required to prevent the extra copies is more expensive than the vectorized
// copy of the CABAC contexts.
//
// We do not save the coefficients of the transform tree (only the transform
// nodes). The rationale is that the final encoding may not use the same
// coefficients as the analysis, and even if it does, it is not clear that the
// reconstruction overhead is higher than the snapshot overhead.
//
typedef struct f265_stash
{
    // Stash control flags:
    // 0-2: stash the YUV components specified.
    //   3: stash the transform nodes.
    //   4: stash the CABAC contexts.
    // This field is zero if the stash is unused.
    uint32_t flags;

    // Stash offsets of the current block as shown above. We depend on the
    // field order of this structure when pushing stash frames.
    int stash_off[3];

    // Location of the current luma block in the frame.
    int px, py, width, height;

    // Current offset in the transform node array (initial state).
    int tn_off;

    // Stash buffer.
    uint8_t buf[F265_STASH_BUF_SIZE];

} f265_stash;

// Rate control information used by the encoder instance.
typedef struct f265_rate_control
{
    // Frame QP.
    int frame_qp;

    // RC method.
    int32_t method;

    // Complexity estimation method.
    int32_t est_method;

    // Bitrate reduction factor.
    float reduction_mult;

    // Exponent used to compute abr rate control convergence window.
    double lt_conv_exp;

    // Minimum number of frames used to compute abr rate control convergence window.
    int32_t lt_conv_min;

    // Number of future frames for which we have rate control statistics. This
    // value is at least 1.
    int32_t next_frames;

    // Sum of the bits in the past frames.
    int64_t past_enc_bits_sum;

    // Sum of the bits that should have been allocated in the past frames.
    int64_t past_exp_bits_sum;

    // Average frame rate.
    double frame_rate;

    // In estimation mode, this is the moving average of the "actual qbits" /
    // "estimated complexity" ratio of the past frames.
    //
    // "Actual qbits" is the QS corresponding to the average QP of the encoded
    // frame multiplied by the number of bits in the frame after encoding.
    //
    // The counters are seeded with a predicted value at the beginning of the
    // stream.
    //
    // The index is the encoded frame type.
    double est_cmpl_sum[3];
    double est_cmpl_count[3];
    double est_cmpl_decay;

    // In reduction mode, we reuse the average QS assigned to each frame by the
    // original encoder. This QS is multiplied by a factor to meet the reduction
    // target. The factor can be computed from the results of a previous frame
    // as follow: actual_qs/decoder_qs * actual_bits/expected_bits. The first
    // ratio is the actual factor used for the previous frame. The second ratio
    // measures the effect of the actual factor.
    //
    // The following values are used to compute the moving average of the
    // factors of the previous frames.
    double dec_qs_sum;
    double dec_qs_count;
    double dec_qs_decay;

    // Sum of the QS assigned to the past frames. This is seeded with a
    // predicted value at the beginning of the stream.
    double qs_sum;

    // Number of samples in the sums above.
    int64_t qs_count;

    // Total number of frames in the stream. 0 if unknown.
    int64_t stream_nb_frames;

    // Interval between two key frames. A value of one means every frame is a
    // key frame.
    int32_t key_interval;

    // Next key frame counter. A value of one means the next frame will be a
    // key frame.
    int32_t key_countdown;

    // True if key frames are IDR frames.
    int8_t key_idr_flag;

    // Target bitrate in bps.
    int64_t bitrate;

    // Minimum and maximum bitrate range allowed for DBRA.
    int64_t dbra_range[2];

    // Maximum and minimum QP.
    int8_t qp_bounds[2];

    // In reduction mode, we reuse the average QS assigned to each frame by the
    // original encoder. This QS is multiplied by a factor to meet the reduction
    // target. The factor can be computed from the results of a previous frame
    // as follow: actual_qs/decoder_qs * actual_bits/expected_bits. The first
    // ratio is the actual factor used for the previous frame. The second ratio
    // measures the effect of the actual factor.
    //
    // The following values are used to compute the moving average of the
    // factors of the previous frames.
    double qs_factor_sum;
    double qs_factor_count;
    double qs_factor_decay;
} f265_rate_control;

// Data used by an encoding thread.
struct f265_enc_thread
{
    // CTB data.
    F265_ALIGN64

    // FIXME: consider importing a subset of gen_data and src_frame here for
    // convenience and avoiding the t->enc indirection.

    // CTB neighbour bitfield, in raster scan: 0 D, 1 B, 2 C, 3 A.
    uint8_t ctb_neighbours;

    // Slice / CTB flags. The values change per slice / CTB.
    uint32_t sflags, mflags;

    // Absolute number of the CTB in raster scan.
    int32_t ctb_xy;

    // CTB index in encoder order.
    int32_t ctb_idx;

    // Position of the CTB in the X, Y direction in terms of CTBs.
    int16_t ctb_x, ctb_y;

    // Offset of the CTB in the X, Y direction in terms of pixels.
    int32_t ctb_off[2];

    // YUV planes stride, in pixels.
    int32_t plane_stride;


    // Pointers to miscellaneous external data.

    // Pointer to the encoder.
    f265_enc *enc;

    // Pointer to the source frame.
    f265_frame *src_frame;

    // Pointer to the reconstruction planes of the current CTB.
    f265_pix *rec_planes[4+2];


    // Lambda/QP cache. FIXME, do we still want a lambda cache now that the QP
    // can change per 8x8 block?

    // True if the cache is valid. The cache is invalidated when the QP or the
    // lambda values change.
    int8_t lambda_cache_valid_flag;

    // HM related fields use to ensure bit-exactness.

    // Lagrangian multiplier used during luma and chroma RDO.
    double hm_lambda[2];

    // Chroma weight factor used to scale the chroma distortion.
    // The HM only uses the above lambda. This is how it deals with
    // the fact that luma and chroma have different importances.
    double hm_wcd;

    // Luma and chroma QP.
    int8_t qp[2];

    // Lambdas for non-rdo/rdo motion estimation. The costs below are computed
    // using the non-rdo lambda.
    int16_t lambdas[2];


    // Analysis data about the current block.
    f265_an_block an;


    // Intra.

    // Maximum number of pixels on the top/left edge of the current CTB that are
    // potentially available for intra prediction, excluding the top-left pixel
    // and without considering the availability of CTB A and B. The values
    // depend only on the location where the padded frame ends relative to the
    // current CTB.
    //
    // Top/left value ranges: [8, 2*ctb_size] / [8, ctb_size].
    //
    // FIXME: consider adding the CTB cutoff in another array.
    int32_t intra_avail_cutoff[2];

    // FIXME: intra prediction map to speed up intra prediction.
    // [   T T ] T: top neighbours.
    // [ L C C ] L: left neighbours.
    // [ L C C ] C: current CB partitions.
    // int8_t imap[3*3];


    // Inter.

    // Number of reference lists.
    int8_t nb_lists;

    // Number of indices in each reference list.
    int8_t nb_ref_idx[2];

    // Motion compensation bounds (X min, Y min, X max, Y max).
    union { int16_t mc_bounds[4]; uint64_t mc_bounds64; };

    // Reference contexts in each reference list.
    F265_ALIGN16 f265_ref_ctx ref_ctx[2][16];

    // Temporal motion vectors of each 16x16 block. This is used for the
    // prediction of a motion vector candidate associated to a reference index
    // RI. The first index is the 16x16 block number in raster scan. The second
    // index is the reference list.
    f265_mv temp_mv[25][2];

    // Temporal reference index. The first index is the 16x16 block number in
    // raster scan. The second index is the reference list.
    // Set to -1 if the MV is invalid.
    int8_t temp_ref_idx[25][2];

    // Distance scale factor for temporal motion vector prediction scaling. The
    // first index is the reference list. The second index is the reference
    // index.
    int16_t temp_dsf[16][2];

    // Distance scale factor for spatial motion vector prediction scaling. The
    // first index is the reference index. The second index is the reference
    // list. The third index is 0 for the current reference (tb), 1 for spatial
    // pmv references (tx).
    int16_t spatial_dsf[16][2][2];

    // Same field as in f265_frame.
    int8_t temp_ref_info;

    // Data about the current inter block.
    f265_inter_block ib;

    // Motion estimation context for the current block.
    f265_me_ctx me;


    // CTB-local data.

    // Coding block data. The index is the CB index. The neighbour CBs and the
    // unavailable CB are appended to the end (top-left, top, top-right, left).
    // Summary: (1 + 4 + 16 + 64) + (1 + 8 + 1 + 8 + 1) = 104.
    f265_cb cb[104];

    // Prediction map of the blocks within and including the borders. The index
    // is the 8x8 block number in raster scan. The stride is 10 and CB 0 is at
    // offset 11. The value is a bitfield.
    // 0-6: coding block index at this location.
    // 7-9: null bits.
    // a-b: CB partition number corresponding to 4x4 block 1.
    // c-d: CB partition number corresponding to 4x4 block 2.
    // e-f: CB partition number corresponding to 4x4 block 3.
    //
    // The prediction map must be updated fully after the CB analysis. During
    // the CB analysis, only the entries in the current CB that are referenced
    // for intra/inter prediction need to be updated.
    //
    // The CTB neighbours and the absent CBs must be correctly initialized.
    // CB_INTRA_FLAG must be set correctly for the current CB. The entries
    // following the current CB in encoding order within the CTB need not be
    // initialized.
    uint16_t pmap[10*10];

    // QP map per 8x8 block inside the CTB and on the top/left borders. The
    // index is the 8x8 block position in raster scan.
    // Index 0 refers to the top-left 8x8 block of the current CTB, while
    // index 10 refers to the first 8x8 block inside the current CTB.
    uint8_t qmap[9*9];

    // Current transform tree pointers (point in the buffers below).
    f265_tt_enc tt;

    // Transform map of the blocks inside the CTB. The values describe the
    // nodes of the transform tree of the coding blocks in encoding order.
    // There are 1 + 4 + 16 + 64 + 256 = 341 nodes in a 64x64 CTB in the worst
    // case, corresponding to a 64x64 block split recursively down to 256 4x4
    // blocks.
    //
    // The value of each node is a bitfield:
    // - Non-zero flags (YUV, 3 bits).
    // - Split flag.
    // - Transform skip flags (YUV, 3 bits).
    //
    // The nodes are laid out in bitstream encoding order. For example, for a
    // 64x64 CTB, the first node corresponds to the 64x64 block. The 64x64
    // block is necessarily split. The second node corresponds to the first
    // 32x32 block. If this block is split, the third node corresponds to the
    // first 16x16 block, otherwise it corresponds to the second 32x32 block.
    //
    // There is no node associated to a split or absent coding block.
    // Otherwise, the first node indicates whether the transform tree is empty.
    // This node is present regardless of the mode of the CB.
    uint8_t tmap[341];

    // Transform block data in encoding order. A transform block only uses an
    // entry if it is non-zero. The worst case is a transform block for each of
    // the 4x4 blocks in a 64x64 CTB and each component (16*16=256).
    f265_tb_enc tb[3*256];

    // Subblock data in encoding order. A subblock only uses an entry if it is
    // non-zero.
    f265_sb_enc sb[3*256];

    // Subblock coefficient levels in encoding order. A subblock only uses an
    // entry if it has remaining levels that must be encoded.
    int16_t levels[3*256*16];


    // Bitstream.

    // Segmentation control flags from the CTB map.
    uint8_t seg_flags;

    // True if we must initialize a segment.
    uint8_t init_segment_flag;

    // Pointer to the number of segments in the section.
    uint32_t *nb_segs;

    // Pointer to the current segment descriptor (flags ctb_address nb_chunks
    // chunk0_size chunk1_size ...)
    uint32_t *seg;

    // Pointer to the chunk buffer.
    uint8_t *chunk_buf;

    // Information about the segment chunk being encoded.
    f265_seg_chunk chunk;

    // CTB CABAC bitstream.
    f265_cabac_bs cbs;

    // Current CTB and WPP CABAC snapshot.
    f265_cabac_snapshot ctb_snap;
    f265_cabac_snapshot wpp_snap;

    // CABAC buffer for the current CTB.
    F265_ALIGN64 uint8_t cabac_buf[F265_CABAC_BUF_SIZE];


    // Analysis stash.
    f265_stash stash;

    // The store buffer contains the large aligned data used by the C and
    // assembly functions. Storage is allocated in a stack-like fashion, in
    // chunks of 64-bytes (cache lines) to respect the alignment requirements.
    // The store buffer pointer points to the current allocation offset in the
    // buffer.
    //
    // The data before the current allocation offset is owned by the pure C
    // functions. The data after the current allocation offset is the spill
    // buffer used by the assembly code (or its C equivalent) to spill vector
    // registers and store temporary data. The spill buffer is trashed at every
    // C-to-assembly call.
    //
    // The store buffer avoids the need to allocate large sparse buffers on the
    // stack to cater for the worst case (e.g. 64x64 blocks). The proximity of
    // the spill buffer with the related pure C function data reduces the cache
    // footprint. Furthermore, the C and the assembly functions need not waste
    // time and space keeping the stack aligned to the ever-growing vector
    // register size. The overhead of passing an extra parameter to an assembly
    // function that needs to spill is minimal. Assembly programming becomes
    // simpler and future-proof.
    //
    // In order to optimize the cache footprint, do not store large data on the
    // stack ever.
    uint8_t *store;
    F265_ALIGN64 uint8_t store_buf[F265_STORE_BUF_SIZE];


    // Deblocking.

    // Mapping between a reference index and a frame identifier. The first index
    // is the reference list. The second index is the reference index plus 1.
    // The value is a unique identifier for the referenced frame, if any.
    int8_t deblock_frame_id_map[2][17];
};

// Invariant data shared by all threads in the encoder instance.
typedef struct f265_gen_data
{
    // True if the encoder uses high bit depths. This field must be the first
    // field of f265_enc because the bit depth dispatching relies on it.
    uint8_t hbd_flag;

    // CTB edge size.
    int8_t ctb_size;

    // Chroma scale factor in the X,Y directions (0 or 1).
    int8_t csf[2];

    // YUV planes stride, in pixels.
    // FIXME: replace by plane_stride.
    int32_t stride;

    // Size of the padded luma/chroma plane buffers.
    int32_t plane_size[2];

    // Offset of the first non-padded pixel of the plane from the luma/chroma
    // plane buffers (YUV).
    int32_t plane_off[3];

    // Number of CTBs.
    int32_t nb_ctb;

    // Frame width and height in number of CTBs.
    int16_t ctb_dim[2];

    // Size allocated per CTB in the chunk buffer.
    int32_t ctb_chunk_size;

    // == Direct parameter import start ==

    // Frame width and height in terms of pixels (multiple of CB size).
    int32_t pix_dim[2];

    // Clipped frame width and height (not a multiple of CB size).
    int32_t clip_dim[2];

    // Number of encoding and lookahead worker threads.
    int8_t nb_workers[2];

    // Multithreading mode for encoding.
    int8_t mt_mode;

    // Chroma format (0 => 4:0:0, 1 => 4:2:0, 2 => 4:2:2, 3 => 4:4:4).
    int8_t chroma_format;

    // Luma, chroma, PCM luma, PCM chroma bit depths.
    int8_t bit_depth[4];

    // CB minimum/maximum log size (3..6).
    int8_t cb_range[2];

    // PCM minimum/maximum log size (3..5, -1: disabled).
    int8_t pcm_range[2];

    // TB minimum/maximum log size (2..5).
    int8_t tb_range[2];

    // Maximum transform depths for intra/inter (0..3).
    int8_t tb_depth[2];

    // Quantization group log size (3..6, -1: disabled).
    int8_t qg_log;

    // Number of reference frames.
    int8_t nb_refs;

    // Number of B frames.
    int8_t nb_b_frames;

    // Profile indicator.
    int8_t profile_idc;

    // Level indicator.
    int8_t level_idc;

    // Chroma QP index offset.
    int8_t chroma_qp_idx_off;

    // Deblocking filter Beta/tC offsets.
    int8_t deblock_off[2];

    // Number of merge candidates.
    int8_t merge_cand;

    // Log 2 of the parallel merge level.
    int8_t parallel_merge_level;

    // == Direct parameter import end ==

    // Initial luma QP.
    int8_t init_qp;

    // Number of bits used for the picture order count.
    int8_t poc_bits;

    // Default number of reference indices in each reference list.
    int8_t default_nb_ref_idx[2];

    // Number of frames to buffer for continuous playback.
    int8_t nb_reordered_frames;

    // Frame rate numerator and denominator. This is used for SPS/PPS
    // initialization, not for rate control since the frame rate is dynamic.
    int32_t frame_rate_num;
    int32_t frame_rate_den;

    // Encoder flags. The values persist for the duration of the stream.
    uint32_t eflags;

    // Chroma QP table. The index is the luma QP. FIXME: extension for high bit
    // depth?
    uint8_t chroma_qp_table[52];

    // This map is used to compute the maximum number of pixels available for
    // intra prediction above/left of a block in the CTB. The index is the
    // raster scan address of a 4x4 block in the CTB. The value is packed as two
    // four-bit fields whose values are within [2,7]. The maximum number of
    // pixels available is 1<<field. For example, for a 64x64 CTB there are up
    // to 128 pixels available on the top edge of the first 4x4 block of the CTB
    // and 4 pixels available on the top edge of the last 4x4 block of the CTB.
    uint8_t intra_avail_map[16*16];

    // Early motion estimation parameters. The index is as follow: 0
    // (non-weighted), 1 (weighted), 2 (subsampled). FIXME, do we still need the
    // weighted prediction entry?
    f265_early_me_params early_me_params[3];

    #ifdef F265_HAVE_STDIO
    // Reconstructed YUV frame file. NULL if not used.
    FILE *yuv_dump_file;
    #endif

    // HM compatibility GOP entries.
    int hm_gop_compat_flag;
    int hm_gop_size;
    f265_hm_gop_entry hm_gop[16];

    // Bitfield used to selectively enable experimental algorithms. The bitfield
    // flags are hardcoded and not documented on purpose.
    uint64_t algo;

} f265_gen_data;

// Data exclusive to the main thread.
typedef struct f265_main_data
{
    // Pointer to the encoding thread array.
    //
    // If threads are not used, the main thread encodes all the slices of the
    // current frame. The first thread object is the main thread.
    //
    // In slice multithreading, the threads encode the slices of the current
    // frame in parallel in random order until all the slices have been encoded.
    // The pointers to the slice data are set in the chunks array of the first
    // thread. The first thread object is the main thread.
    //
    // In frame multithreading, the frames are encoded in parallel. A thread
    // encodes all the slices of its frame. Instead of encoding frames, the main
    // thread waits for the oldest frame in coded order to be encoded. For that
    // purpose, the encoding threads are placed at the beginning of the array in
    // coded order, i.e. leftmost is oldest. The idle threads are stored at the
    // end of the array.
    f265_enc_thread **enc_threads;

    // Pointer to the array of frames being encoded, in coded order. For
    // unification, the first element is the last encoded frame.
    f265_frame **enc_frames;

    // Pointer to the last frame used for reference by the lookahead.
    f265_frame *la_ref_frame;

    // Current request.
    f265_enc_req *req;

    // Absolute picture order counter.
    int64_t abs_poc_counter;

    // Number of frames encoded so far.
    int64_t nb_past_frames;

    // Number of encoding thread objects.
    int16_t nb_enc_threads;

    // Number of frames being encoded by a thread.
    int16_t nb_enc_frames;

    // Number of non-null frames in the lookahead.
    int16_t nb_la_frames;

    // Same value as in the lookahead, stored here to avoid sharing issues.
    int16_t la_process_delay;

    // True if the encoder must exit. This is set in mutual exclusion by
    // locking es.mutex and la.mutex in this order.
    int8_t enc_exit_flag;

    // Arrays of unused objects (frame, src, la, enc).
    uint8_t **unused_objs[4];

    // Count of unused objects.
    int16_t nb_unused_objs[4];

    // Rate control.
    f265_rate_control rc;

    #ifdef VAN_USE_HM_ME
    // Buffers to convert 8u to 16s for bi-dir inter.
    f265_pix *hm_ref_plane_16s[2][16];
    f265_pix *hm_src_plane_16s;
    f265_pix *hm_new_hm_src_plane_16s;
    f265_pix *hm_ref_dual_plane_16s;
    int64_t hm_last_poc;
    #endif

} f265_main_data;

// The lookahead gathers information about the upcoming frames to encode.
//
// When the lookahead analyzes a frame, the lookahead determines a sequence of
// frames to go from the oldest frame (the frame to encode next) up to the
// analyzed frame. The information about that sequence is stored in the oldest
// frame.
//
// The lookahead and the main thread run at varying speeds. To prevent the
// lookahead from starving the main thread, the lookahead analyzes frames in
// advance and stores them in an output queue until they are consumed by the
// main thread. The output does not depend on the lookahead speed since the
// lookahead processes frames in a deterministic fashion.
//
// The lookahead modifies the internal frame data as it works. Until a frame is
// outputted, the main thread is forbidden to access that information.
//
// The lookahead is flushed by submitting null frames until every frame has
// exited the lookahead. The lookahead buffers frames by outputting null frames
// until the lookahead buffer is full. The lookahead handle spurious flushes
// (null frames followed by non-null frames) gracefully.
//
// The lookahead uses slice threads to analyze frames. The main thread
// synchronizes with the primary thread (the first worker thread) and the
// primary thread synchronizes with the other worker threads.
typedef struct f265_lookahead
{
    // Data exclusive to the lookahead and invariant during slice encoding.

    // Pointer to the array of frames stored in display order. For unification,
    // the first element is the last frame of the last committed group of
    // pictures.
    f265_frame **display;

    // Pointer to the array of frames stored in coded order. The last committed
    // group of pictures is at the beginning.
    f265_frame **coded;

    // Interval between two key frames. A value of one means every frame is a
    // key frame.
    int32_t key_interval;

    // Key frame type.
    int32_t key_type;

    // Next key frame counter. A value of one means the next frame will be a
    // key frame.
    int32_t key_countdown;

    // True if key frames are IDR frames.
    int8_t key_idr_flag;

    // Number of frames processed in advance by the lookahead. This is 0 when
    // the main thread consumes a frame as soon as the lookahead analyzes it.
    int16_t process_delay;

    // Number of frames that must have been analyzed (excluding the current
    // one) before the lookahead commits a group of pictures. This is 0 if the
    // lookahead commits a frame has soon as it is received (not accounting for
    // process_delay). This value must be one more than the maximum number of B
    // frames.
    int16_t decision_delay;

    // Number of frames in the display array, excluding the first element.
    int16_t nb_undecided;

    // Number of committed frames in the coded array.
    int16_t nb_committed;

    // Maximum number of B frames.
    int8_t nb_b_frames;

    // Number of lookahead thread objects.
    int8_t nb_la_threads;


    // Data shared between the main thread and the lookahead.

    // Input/output frame queues between the main thread and the lookahead. Both
    // queues may contain null frames for flushing/buffering purposes.
    //
    // Invariant: every frame (null or non-null) that enters the lookahead
    // eventually gets out.
    f265_frame **in_queue, **out_queue;
    int16_t in_queue_len, out_queue_len;

} f265_lookahead;

// Encoder instance.
struct f265_enc
{
    // Invariant data shared by all threads in the encoder instance.
    F265_ALIGN64 f265_gen_data gd;

    // Main thread data.
    F265_ALIGN64 f265_main_data md;

    // Lookahead data.
    F265_ALIGN64 f265_lookahead la;
};

// Large tables initialized when the library is loaded. FIXME: change name, this
// might not be large anymore and this is bit-depth-dependent.
typedef struct f265_large_tables
{
    // Pixel block function array.
    f265_block_funcs bfa[3*7 + 4];

    // Default weights for weighted prediction.
    f265_weight wp_default[3];

} f265_large_tables;

extern f265_large_tables f265_lt;


///////////////////////////////////////////////////////////////////////////////
// Inline functions.

// FIXME, do we still need the list stuff?

// Initialize a list node. The data field is not set.
finline void venc_init_node(f265_list_node *node)
{
    node->next = node->prev = node;
}

// Add a node before the current node.
finline void venc_prepend_node(f265_list_node *cur, f265_list_node *node)
{
    node->prev = cur->prev;
    node->next = cur;
    cur->prev->next = node;
    cur->prev = node;
}

// Remove a node from a list and return the data pointer.
finline void* venc_remove_node(f265_list_node *node)
{
    node->next->prev = node->prev;
    node->prev->next = node->next;
    return node->data;
}

// Return true if a list is empty.
finline int venc_is_list_empty(f265_list_node *node)
{
    return (node->next == node);
}

// Initialize a variable number of constant-sized blocks to zero. 'dst' is the
// destination, which can be unaligned. 'size' is a constant expression giving
// the size of the blocks. 'count' is the number of blocks.
//
// This macro should be used when the number of blocks is variable to prevent
// gcc from calling memset() from the PLT. Use the regular memset() function
// when the total size is known.
finline void venc_zero(void *dst, int32_t size, int32_t count)
{
    memset(dst, 0, size*count);
}

// Same as above for memcpy().
finline void venc_copy(void *dst, void *src, int32_t size, int32_t count)
{
    memcpy(dst, src, size*count);
}

// FIXME: will need a variable-size memset() replacement as well.

// Helper function for f265_mv_out_of_range().
static finline uint32_t venc_pack_mv_range(int32_t mx, int32_t my)
{
    return (mx&0x7fff) + (my<<16);
}

// This function tests if the motion vector specified is outside the packed
// bounds specified.
//
// The frame is padded, so MinX and MinY are always negative and MaxX and MaxY
// are always positive. Both the X and the Y components can be represented in
// less than 13 bits.
//
// The packing function puts the Y component in the high 16 bits and the X
// component in the low 16 bits. It also clears bit 15 to avoid contamination
// between the components. The function tests for underflow when the MV bounds
// are exceeded.
//
// In the case of MinX, the function adds Mx and -MinX. If Mx < MinX, the sum is
// negative and the sign bit 14 is set, otherwise it is cleared. The same
// applies for MinY with bit 31. Notice that the underflow detection for Mx and
// My are independent of each other.
//
// In the case of MaxX/MaxY, the function subtracts My from MaxY and Mx from
// MaxX|0x8000. If My > MaxY, the difference is negative and bit 31 is set. If
// My = MaxY, the difference depends on the value of the low 16 bits. Since bit
// 15 is set for MaxX and cleared for Mx, the difference is zero in the high 16
// bits. Thus, if My > MaxY, bit 31 is set independently of the value of Mx.
// Otherwise, bit 31 is cleared and the difference in the low 16 bits is
// independent of the value of My. If Mx > MaxX, bit 15 is cleared for the carry
// and bit 14 is set, otherwise bit 15 is set and bit 14 is cleared.
//
// Example with MinX = -2, MaxX = 2, MinY = -2, MaxY = 2:
//
// MinX: [yyyyy|00010]   MinX: [yyyyy|00010]   MinY: [00010|0xxxx]   MinY: [00010|0xxxx]
// + -3: [yyyyy|01101]   + -2: [yyyyy|01110]   + -3: [11101|0xxxx]   + -2: [11110|0xxxx]
// ===================   ===================   ===================   ===================
//       [yyyyy|01111]         [yyyyy|10000]         [11111|xxxxx]         [00000|xxxxx]
//            Set^              Cleared^           Set^              Cleared^
//
// MaxX: [yyyyy|10010]   MaxX: [yyyyy|10010]   MaxY: [00010|1xxxx]   MaxY: [00010|1xxxx]
// -  3: [yyyyy|00011]   -  2: [yyyyy|00010]   -  3: [00011|0xxxx]   -  2: [00010|0xxxx]
// ===================   ===================   ===================   ===================
//       [yyyyy|01111]         [yyyyy|10000]         [11111|xxxxx]         [00000|xxxxx]
//            Set^              Cleared^           Set^              Cleared^
//
static finline uint32_t venc_mv_out_of_range(f265_mv mv, uint32_t packs[2])
{
    uint32_t pack = venc_pack_mv_range(mv.x, mv.y);
    return ((pack+packs[0])|(packs[1]-pack))&0x80004000;
}

// FIXME: replace legacy interface above.
// FIXME: do something about bounds expressed in qpel.
static finline uint32_t venc_mv_out_of_range2(int mx, int my, uint32_t packs[2])
{
    uint32_t pack = venc_pack_mv_range(mx, my);
    return ((pack+packs[0])|(packs[1]-pack))&0x80004000;
}

// Clip the quarterpel motion vector specified.
static finline f265_mv f265_clip_mv(f265_mv mv, uint64_t mv_bounds64)
{
    // Port assembly code here.
    int16_t min_x = ((mv_bounds64>>0)&0xffff);
    int16_t min_y = ((mv_bounds64>>16)&0xffff);
    int16_t max_x = ((mv_bounds64>>32)&0xffff);
    int16_t max_y = ((mv_bounds64>>48)&0xffff);
    mv.x = F265_MIN(F265_MAX(mv.x, min_x), max_x);
    mv.y = F265_MIN(F265_MAX(mv.y, min_y), max_y);
    return mv;
}

// RDO CABAC encoding functions.
static finline void venc_encode_context_bin_rdo(f265_cabac_bs *cbs, int ctx_idx, int bin)
{
    int context = cbs->contexts[ctx_idx];
    cbs->contexts[ctx_idx] = f265_cabac_transit_table[context][bin];
    cbs->rdo.bits += f265_cabac_entropy_table[context^bin];
}

static finline void venc_encode_bypass_bin_rdo(f265_cabac_bs *cbs, int bin)
{
    cbs->rdo.bits += 32768;
}

static finline void venc_encode_bypass_bins_rdo(f265_cabac_bs *cbs, int bins, int nb_bins)
{
    cbs->rdo.bits += 32768*nb_bins;
}

static finline void venc_encode_term0_bin_rdo(f265_cabac_bs *cbs)
{
    cbs->rdo.bits += f265_cabac_entropy_table[126];
}

#ifdef VAN_TRACE_SYNTAX
extern int venc_trace_syntax_flag;
// Announce the next syntax element.
static inline void venc_trace_syntax_element(const char *what)
{
    if (venc_trace_syntax_flag) printf("  %s\n", what);
}
#else
static inline void venc_trace_syntax_element(const char *what) {}
#endif

// Get the offset of a CB block in a block map.
static finline int venc_get_block_map_offset(f265_enc_thread *t, f265_cb *cb, int cb_ox, int cb_oy,
                                             int shift, int stride)
{
    return ((t->ctb_off[1] + cb->cb_off[1] + cb_oy)>>shift)*stride + ((t->ctb_off[0] + cb->cb_off[0] + cb_ox)>>shift);
}


///////////////////////////////////////////////////////////////////////////////
// Global functions.

// analyze.c
int venc_analyze_ctb(f265_enc_thread *t);
int venc_hm_an_ctb(f265_enc_thread *t);
void venc_update_se_costs(f265_enc_thread *t);

// bs.c
void venc_vlc_put_bit(f265_vlc_bs *vbs, uint32_t code);
void venc_vlc_put_bits(f265_vlc_bs *vbs, uint64_t code, int length);
void venc_vlc_put_vlc(f265_vlc_bs *vbs, uint32_t code);
void venc_vlc_flush(f265_vlc_bs *vbs);
void venc_write_vps(f265_vlc_bs *vbs, f265_enc *enc);
void venc_write_sps(f265_vlc_bs *vbs, f265_enc *enc);
void venc_write_pps(f265_vlc_bs *vbs, f265_enc *enc);
void venc_write_slice_header(f265_vlc_bs *vbs, f265_enc *enc, f265_frame *f, int *chunk_sizes, int nb_chunks,
                             uint32_t seg_flags, int ctb_xy, int nal_type);
void venc_begin_nal(f265_nal_bs *nbs, int nal_type, int zero_byte_flag);
void venc_finish_nal(f265_nal_bs *nbs, int data_size);
void venc_begin_vlc(f265_vlc_bs *vbs, uint8_t *base);
void venc_write_pure_vlc_nal(f265_nal_bs *nbs, f265_enc *enc, int nal_type,
                             void (*data_writer)(f265_vlc_bs*, f265_enc*));
void venc_write_segment_nal(f265_nal_bs *nbs, f265_enc *enc, f265_frame *f, uint32_t **seg, uint8_t **chunks,
                            int nal_type, int zero_byte_flag);
void venc_encode_frame_nals(f265_enc *enc, f265_frame *f, uint8_t *chunk_buf);
void venc_init_cabac_engine(f265_cabac_bs *cbs, uint8_t *buf);
void venc_init_cabac_contexts(f265_cabac_bs *cbs, int slice_type, int qp);
void venc_load_cabac_engine(f265_cabac_bs *cbs, f265_cabac_snapshot *snapshot);
void venc_load_cabac_contexts(f265_cabac_bs *cbs, f265_cabac_snapshot *snapshot);
void venc_load_cabac(f265_cabac_bs *cbs, f265_cabac_snapshot *snapshot);
void venc_save_cabac_engine(f265_cabac_bs *cbs, f265_cabac_snapshot *snapshot);
void venc_save_cabac_contexts(f265_cabac_bs *cbs, f265_cabac_snapshot *snapshot);
void venc_save_cabac(f265_cabac_bs *cbs, f265_cabac_snapshot *snapshot);
void venc_write_cabac_chunk(f265_cabac_bs *cbs);
void venc_renormalize_cabac(f265_cabac_bs *cbs);
void venc_encode_context_bin_raw(f265_cabac_bs *cbs, int ctx_idx, int bin);
void venc_encode_bypass_bin_raw(f265_cabac_bs *cbs, int bin);
void venc_encode_bypass_bins_raw(f265_cabac_bs *cbs, int bins, int nb_bins);
void venc_encode_term0_bin_raw(f265_cabac_bs *cbs);
void venc_encode_term1_bin(f265_cabac_bs *cbs);
void venc_get_chunk_buf_offsets(f265_enc *enc, uint8_t *buf, uint32_t **nb_segs, uint32_t **seg, uint8_t **chunks,
                                int ctb_idx, int nb_ctbs);
void venc_init_segment(f265_enc_thread *t, int dependent_flag);
void venc_init_seg_chunk(f265_seg_chunk *chunk, uint8_t *base);
void venc_write_stream_end(f265_cabac_bs *cbs, int segment_end_flag, int chunk_end_flag);
void venc_write_seg_chunk(f265_cabac_bs *cbs, f265_seg_chunk *chunk, int chunk_end_flag);
void venc_write_ctb_chunk(f265_enc_thread *t, f265_seg_chunk *chunk, int segment_end_flag, int chunk_end_flag);
void venc_commit_seg_chunk(f265_enc_thread *t, f265_seg_chunk *chunk, int segment_end_flag, int chunk_end_flag);

// deblock.c
void venc_save_deblock_tt(f265_enc_thread *t, f265_cb *cb, int lg_bs, int cb_ox, int cb_oy);
void venc_save_deblock_pb(f265_enc_thread *t, f265_cb *cb);
void venc_build_deblock_frame_id_map(f265_enc_thread *t);
void venc_deblock_frame(f265_enc_thread *t);

// enc.c
void venc_dump_yuv_file(FILE *file, f265_pix *planes[3], int64_t frame_poc, int stride, int width, int height,
                        int sx, int sy);
void venc_deinit_enc(f265_enc *enc);
void venc_analyze_params(f265_enc_params *params);
void venc_init_enc(f265_enc_params *params, uint8_t *buf, f265_enc **enc_handle, char **error_handle);
uint8_t* venc_get_unused_obj(f265_enc *e, int cat);
void venc_link_src_obj(f265_enc *e, f265_frame *f);
void venc_link_enc_obj(f265_enc *e, f265_frame *f);
void venc_release_frame_mem(f265_enc *e, f265_frame *f);
int venc_process_enc_req(f265_enc *e, f265_enc_req *req);
int venc_get_ctb_block_plane_off(f265_enc_thread *t, int comp, int ct_ox, int ct_oy);
int venc_get_cb_block_plane_off(f265_enc_thread *t, f265_cb *cb, int comp, int cb_ox, int cb_oy);
int venc_get_depth_scan_idx(int x, int y, int size);
int venc_get_cb_part_mode(f265_cb *cb);
void venc_get_cb_loc4(int cb_loc[2], f265_cb *cb);
void venc_get_part_loc4(int part_size[2], int part_off[2], int part_mode, int part_idx, int cb_size);
void venc_get_part_loc(int part_size[2], int part_off[2], int part_mode, int part_idx, int cb_size);
void venc_update_pmap_unsplit_cb(f265_enc_thread *t, f265_cb *cb);
void venc_update_pmap_cb(f265_enc_thread *t, f265_cb *cb);
void venc_lookup_pmap(f265_enc_thread *t, int x, int y, int *cb_idx, int *part_idx);
int venc_load_ctb_neighbour_cb(f265_enc_thread *t, int *next_cb_idx, int n_off, int loc);
void venc_load_ctb_neighbours(f265_enc_thread *t);
void venc_save_cb(f265_enc_thread *t, f265_cb *cb);
void van_verify_analysis_save(f265_enc_thread *t, int loc);
void van_verify_analysis_check(f265_enc_thread *t);

#ifdef VAN_VERIFY_RDO_ANALYSIS
extern f265_pix venc_verify_analysis_rec[2][3][64*64];
extern uint8_t venc_verify_analysis_ctx[2][F265_NB_CABAC_CTX];
void venc_verify_analysis_save(f265_enc_thread *t, int loc);
void venc_verify_analysis_check(f265_enc_thread *t);
#endif

// entropy.c
void venc_write_ctb(f265_enc_thread *t);

// hm.c
void venc_show_loc(f265_enc_thread *t, int ct_ox, int ct_oy, int size);
void venc_load_hm_ctb_analysis(f265_enc_thread *t);
void venc_parse_hm_gop_file(f265_enc *enc, char *path);
void venc_hm_reorder_gop(f265_enc *enc, int *nb_gop, int *key_flag);
void venc_hm_set_intra(f265_enc_thread *t);
void venc_hm_check_rec(f265_enc_thread *t);
#ifdef VAN_VALIDATE_MODE_PRED
void venc_hm_validate(f265_enc_thread *t);
#endif

// inter.c
void venc_mc_cb(f265_enc_thread *t, f265_cb *cb, f265_pix *dst, int dst_stride, int comp);
void venc_mc_luma_p(f265_enc_thread *t, f265_pix *dst, int dst_stride, f265_ref_ctx *rc, f265_mv mv,
                    int packed_dims, int plane_off);
void venc_mc_chroma_p(f265_enc_thread *t, f265_pix *dst, int dst_stride, f265_ref_ctx *rc, f265_mv mv,
                      int packed_dims, int plane_off, int comp);
void venc_mc_luma_b(f265_enc_thread *t, f265_pix *dst, int dst_stride, f265_ref_ctx *rc[2], f265_mv mv[2],
                    int packed_dims, int plane_off);
void venc_mc_chroma_b(f265_enc_thread *t, f265_pix *dst, int dst_stride, f265_ref_ctx *rc[2], f265_mv mv[2],
                      int packed_dims, int plane_off, int comp);

// intra.c
void venc_get_intra_filter_flags(int *filter_edge_flag, int *filter_neighbour_flag, int *neighbour_bilinear_flag,
                                 int comp, int lg_bs, int mode, int smooth_intra_flag);
void venc_get_intra_encode_flags(int *dst_flag, int *order, int comp, int lg_bs, int mode);
void venc_predict_intra_neighbours(f265_enc_thread *t, f265_pix dst[129], int rec_flag,
                                   int comp, int bs, int ct_ox, int ct_oy);
void venc_filter_intra_neighbours(f265_pix *dst, f265_pix *src, int bs, int bd, int bilinear_flag);
void venc_predict_intra_planar(f265_pix *dst, f265_pix *nbuf, int lg_bs);
void venc_predict_intra_dc(f265_pix *dst, f265_pix *nbuf, int lg_bs, int filter_edge_flag);
void venc_predict_intra_angular(f265_pix *dst, f265_pix *nbuf, int lg_bs, int bd, int filter_edge_flag, int mode);
void venc_predict_intra_mode(f265_pix *dst, f265_pix *neighbours, int lg_bs, int bd, int mode, int filter_edge_flag);
void venc_predict_intra(f265_enc_thread *t, f265_pix *dst, int comp, int lg_bs, int mode, int ct_ox, int ct_oy);
void venc_get_intra_pred_mode(f265_enc_thread *t, f265_cb *cb, int partition_idx, int *mpm_list);

// inter.h
void venc_mc_cb(f265_enc_thread *t, f265_cb *cb, f265_pix *dst, int dst_stride, int comp);
void venc_frame_set_up_inter_pred(f265_enc_thread *t);
void venc_ctb_set_up_inter_pred(f265_enc_thread *t);
void venc_get_neighbour_mvs(f265_enc_thread *t, f265_cb *cb, uint32_t partition_idx,
                            f265_inter_neighbour_mv *neighbours);
void venc_get_pmv(f265_enc_thread *t, int part_idx, f265_inter_neighbour_mv *neighbours,
                  uint32_t ref_idx, uint32_t reference_list, f265_mv *pmv);
void venc_get_merge_candidate(f265_enc_thread *t, f265_cb *cb, uint32_t partition_idx,
                              f265_inter_neighbour_mv *neighbours, f265_inter_neighbour_mv *merge_candidate);

// la.c
f265_frame* venc_la_make_frame(f265_enc *e);
f265_frame* venc_la_process_frame_regular(f265_enc *enc, f265_frame *in);
void venc_la_process_frames(f265_enc *enc);
f265_frame* venc_la_add_frame(f265_enc *e, f265_frame *in);

// me.c
void venc_me_interpol_plane(int16_t *dst, int dst_stride, f265_mv mv, f265_me_ctx *me, int csf[2],
                            f265_pix *ref_plane, int comp, int width, int height);
void venc_me_interpol(f265_pix *dst, int dst_stride, f265_mv mv, f265_me_ctx *me, int comp);
void venc_me_interpol_bi(f265_pix *dst, int dst_stride, f265_mv mv[2], f265_me_ctx *me, int comp);
int venc_me_get_dist(f265_me_ctx *me, f265_pix *src0, int32_t stride0, f265_pix *src1,
                     int32_t stride1, int32_t width, int32_t height, int32_t bitdepth);
int venc_me_luma_cost(f265_mv mv, f265_me_ctx *me);
int venc_me_chroma_cost(f265_mv mv, f265_me_ctx *me, int comp);
int venc_me_merge_cand_dist(f265_enc_thread *t, f265_inter_neighbour_mv cand);
int venc_me_mv_cost(f265_mv mv, uint16_t *cost_table[2]);
int venc_me_mv_cost_test(f265_me_ctx *me, f265_mv mv, int ref_id);
int venc_me_mv_total_cost(f265_mv mv, f265_me_ctx *me);
int venc_me_mv_total_cost_bi(f265_mv mv[2], f265_me_ctx *me);
int venc_me_test_pmv(f265_mv pmv[2], f265_me_ctx *me, int dist, int *cost);
int venc_me_find_nearest_pmv(f265_mv pmv[2], uint16_t base_costs[2], f265_me_ctx *me);
void venc_me_keep_best_mv(f265_mv cand_mv, f265_me_ctx *me);
void venc_me_test_all_positions(f265_mv base_mv, f265_me_ctx *me, const int8_t offset[][2], int nb_offset);
void venc_me_dia_search(int32_t nb_iter, int32_t scale, f265_me_ctx *me);
void venc_me_xdia_search(int32_t nb_iter, int32_t scale, f265_me_ctx *me);
void venc_me_hex_search(int32_t nb_iter, int32_t scale, f265_me_ctx *me);
void venc_me_square_search(int32_t nb_iter, int32_t scale, f265_me_ctx *me);
int venc_me_remove_duplicate(f265_mv mv_list[], int nb_mv);
void venc_early_me(f265_me_ctx *me,
                   f265_mv mv_candidate_list[],
                   int nb_candidate,
                   int *threshold);
int32_t venc_sad_wrap(f265_pix *src0, int32_t stride0, f265_pix *src1, int32_t stride1,
                      int32_t width, int32_t height, int32_t bitdepth);
void venc_me_set_params(f265_enc *enc, f265_me_ctx *me);
void venc_me_set_partition(f265_enc_thread *t, f265_cb *cb, int part_idx);
void venc_me_set_ref(f265_enc_thread *t, f265_ref_ctx *ref_ctx, int ref_id);
void venc_me_set_pmv(f265_me_ctx *me, f265_mv pmv, int qp, int ref_id);

// me_hm.c
int32_t venc_hm_bits_count(int32_t val);
int32_t venc_hm_bits_cost(int32_t bits, int32_t lambda);
int32_t venc_hm_mv_cost(int32_t x, int32_t y, f265_mv *mvp, int32_t lambda, int32_t mv_scale);
void venc_hm_select_mvp(f265_enc_thread *t, f265_mv mvp_list[], int32_t *mvp_idx,
                        int32_t *mv_bits, int32_t *bits, int32_t *cost);
int32_t venc_hm_test_mvp(f265_enc_thread *t, f265_cb *cb, f265_mv mvp_list[], f265_frac_ref_block *rb);
void venc_hm_mv_clip(f265_enc_thread *t, f265_cb *cb, f265_mv *mv, int32_t off);
void venc_hm_me_range(f265_enc_thread *t, f265_cb *cb, f265_mv *mv, int32_t range);
void venc_hm_copy_8u_to_16s(f265_pix *dst, int32_t stride_dst, uint8_t *src, int32_t stride_src,
                            int32_t width, int32_t height);
void venc_hm_me_full_search(f265_enc_thread *t, f265_frac_ref_block *rb);
int32_t venc_hm_frac_refine(f265_enc_thread *t, f265_frac_ref_block *rb, f265_pix us_ref[][64 * 64],
                            int32_t stride_ref, f265_mv *frac, int32_t frac_step);
void venc_hm_upsample_all(f265_enc_thread *t, f265_cb *cb, f265_frac_ref_block *rb,
                          f265_pix us_ref[][64 * 64], f265_mv *frac, int32_t frac_step);
int32_t venc_hm_frac_search(f265_enc_thread *t, f265_cb *cb, f265_frac_ref_block *rb, f265_mv *frac);
void venc_hm_tz_diamond(f265_enc_thread *t, f265_frac_ref_block *rb, int32_t start_x, int32_t start_y, int32_t dist);
void venc_hm_tz_2points(f265_enc_thread *t, f265_frac_ref_block *rb );
void venc_hm_tz_search (f265_enc_thread *t, f265_frac_ref_block *rb, f265_cb *cb);
void venc_hm_tz_eval_mv(f265_enc_thread *t, f265_frac_ref_block *rb, int32_t x, int32_t y, int32_t dir, int32_t dist);
void venc_hm_preproc_src(f265_enc_thread *t, f265_frac_ref_block *rb, f265_pix *new_src, int32_t stride_new_src);
void venc_hm_me_search(f265_enc_thread *t, f265_cb *cb, f265_frac_ref_block *rb, int32_t bipred,
                       f265_mv *mv_init, int32_t *mv_bits, int32_t *bits, int32_t *cost);
void venc_hm_upsample(f265_enc_thread *t, f265_cb *cb, f265_frac_ref_block *rb,
                      f265_mv *mv, f265_pix *us_ref, int32_t stride);
int32_t venc_hm_search_inter_cb(f265_enc_thread *t, f265_cb *cb, int32_t part_idx, int32_t *prev_pred_mode);
void venc_hm_analyze_inter_cb(f265_enc_thread *t, f265_cb *cb);
void venc_hm_analyze_inter_ctb(f265_enc_thread *t);
void venc_hm_setup_me(f265_enc_thread *t, f265_cb *cb, int32_t part_height, int32_t bipred_flag,
                      int32_t list, int32_t ref_idx, f265_weight *wp);
void venc_hm_pred_mode_bits(int32_t part_mode, int32_t is_inter, int32_t part_idx,
                            int32_t prev_mode, int32_t pred_mode_bits[]);

// pixel.c
void venc_copy_block(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int width, int height);
void venc_copy_block_s16(int16_t *dst, int dst_stride, int16_t *src, int src_stride, int width, int height);
void venc_transpose_block(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int width, int height);
void venc_pad_plane(f265_pix *plane, int32_t stride,
                    int32_t bl, int32_t br, int32_t bt, int32_t bb,
                    int32_t pl, int32_t pr, int32_t pt, int32_t pb);
void venc_pad_rec_yuv_planes(f265_enc_thread *t);
void venc_compute_hpel(f265_pix *planes[4], int stride, int width, int height, uint8_t *spill);
void venc_compute_hpel_planes(f265_enc_thread *t);
void venc_filter_int(f265_pix *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                     int32_t width, int32_t height, int32_t shift);
void venc_filter_hor_luma(f265_pix *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                          int32_t width, int32_t height, int8_t *coef, int32_t shift);
void venc_filter_ver_luma(f265_pix *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                          int32_t width, int32_t height, int8_t *coef, int32_t shift);
void venc_filter_ver_luma_s16(int16_t *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                              int32_t width, int32_t height, int8_t *coef, int32_t shift);
void venc_filter_hor_chroma(f265_pix *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                            int32_t width, int32_t height, int8_t *coef, int32_t shift);
void venc_filter_ver_chroma(f265_pix *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                            int32_t width, int32_t height, int8_t *coef, int32_t shift);
void venc_filter_ver_chroma_s16(int16_t *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                                int32_t width, int32_t height, int8_t *coef, int32_t shift);
void venc_interpol_luma(f265_pix *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                        int32_t width, int32_t height, int32_t xfrac, int32_t yfrac, int32_t bitdepth);
void venc_interpol_chroma(f265_pix *src, int32_t src_stride, int16_t *dst, int32_t dst_stride,
                          int32_t width, int32_t height, int32_t xfrac, int32_t yfrac, int32_t bitdepth);
void venc_scale_pix_uni(int16_t *src, int32_t src_stride, f265_pix *dst, int32_t dst_stride,
                        int32_t width, int32_t height, int32_t bitdepth);
void venc_scale_pix_bi(int16_t *src0, int16_t *src1, int32_t src_stride, f265_pix *dst, int32_t dst_stride,
                       int32_t width, int32_t height, int32_t bitdepth);
void venc_weight_pix_uni(int16_t *src, int32_t src_stride, f265_pix *dst, int32_t dst_stride,
                         int32_t width, int32_t height, f265_weight *weights, int32_t bitdepth);
void venc_weight_pix_bi(int16_t *src0, int16_t *src1, int32_t src_stride, f265_pix *dst, int32_t dst_stride,
                        int32_t width, int32_t height, f265_weight *weights0, f265_weight *weights1, int32_t bitdepth);
void venc_ref_weighted_pred(int16_t *src0, int16_t *src1, int32_t src_stride, f265_pix *dst, int32_t dst_stride,
                            int32_t bitdepth, f265_frac_ref_block *rb);
void venc_get_frac_ref_luma(f265_enc *enc, f265_pix *dst, int32_t dst_stride, f265_frac_ref_block *rb);
void venc_get_frac_ref_chroma(f265_enc *enc, f265_pix *dst, int32_t dst_stride, f265_frac_ref_block *rb);
void venc_get_frac_ref(f265_enc *enc, f265_pix *dst, int32_t dst_stride, f265_frac_ref_block *rb);
void venc_interpol_luma_qpel_pix_h_c(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                     int packed_dims, uint8_t *spill);
void venc_interpol_luma_qpel_s16_h_c(int16_t *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                     int packed_dims, uint8_t *spill);
void venc_interpol_luma_qpel_pix_v_c(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                     int packed_dims, uint8_t *spill);
void venc_interpol_luma_qpel_s16_v_c(int16_t *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                     int packed_dims, uint8_t *spill);
void venc_interpol_luma_qpel_pix_d_c(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                     int packed_dims, uint8_t *spill);
void venc_interpol_luma_qpel_s16_d_c(int16_t *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                     int packed_dims, uint8_t *spill);
void venc_interpol_chroma_qpel_pix_h_c(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                       int packed_dims, uint8_t *spill);
void venc_interpol_chroma_qpel_s16_h_c(int16_t *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                       int packed_dims, uint8_t *spill);
void venc_interpol_chroma_qpel_pix_v_c(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                       int packed_dims, uint8_t *spill);
void venc_interpol_chroma_qpel_s16_v_c(int16_t *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                       int packed_dims, uint8_t *spill);
void venc_interpol_chroma_qpel_pix_d_c(f265_pix *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                       int packed_dims, uint8_t *spill);
void venc_interpol_chroma_qpel_s16_d_c(int16_t *dst, int dst_stride, f265_pix *src, int src_stride, int frac,
                                       int packed_dims, uint8_t *spill);
void venc_scale_qpel_c(int16_t *dst, int dst_stride, f265_pix *src, int src_stride, int packed_dims);
void venc_avg_pix_c(f265_pix *dst, f265_pix *src0, int src0_stride, f265_pix *src1, int src1_stride, int packed_dims);
void venc_avg_pix_s16_c(f265_pix *dst, int dst_stride, int16_t *src0, int16_t *src1, int src_stride, int packed_dims);
int32_t venc_sad(f265_pix *src0, int32_t stride0, f265_pix *src1, int32_t stride1,
                 int32_t width, int32_t height, int32_t subshift, int32_t bitdepth);
int venc_fsad_c(f265_pix *src0, int stride0, f265_pix *src1, int stride1, int packed_dims);
void venc_sad3_c(int *costs, f265_pix *src, int src_stride, f265_pix **refs, int ref_stride, int packed_dims);
void venc_sad4_c(int *costs, f265_pix *src, int src_stride, f265_pix **refs, int ref_stride, int packed_dims);
int32_t venc_ssd(f265_pix *src0, int32_t stride0, f265_pix *src1, int32_t stride1,
                 int32_t width, int32_t height, int32_t subshift, int32_t bitdepth);
int32_t venc_ssd16(int16_t *src0, int32_t stride0, int16_t *src1, int32_t stride1,
                   int32_t width, int32_t height, int32_t subshift, int32_t bitdepth);
int32_t venc_satd(f265_pix *src0, int32_t stride0, f265_pix *src1, int32_t stride1,
                  int32_t width, int32_t height, int32_t bitdepth);

// rc.c
int venc_rc_check_frame(f265_enc_thread *t);
int64_t venc_rc_get_bitrate(f265_enc* enc);
int64_t venc_rc_set_bitrate(f265_enc* enc, int64_t bitrate);
void venc_rc_init_stream( f265_enc *enc, f265_enc_params *params);
int8_t venc_rc_frame_start(f265_enc_thread *t, f265_frame *prev);
void venc_rc_frame_end(f265_enc_thread *t, int32_t actual_bits, float avg_qp);

// rec.c
void venc_init_transform_tree(f265_enc_thread *t);
void venc_preprocess_tb(f265_tt_enc *tt, int16_t *qc);
int venc_rec_block(f265_rec_params *rp);
int venc_rec_tb(f265_enc_thread *t, f265_pix *pred, int pred_stride, int comp, int lg_bs, int dst_flag, int order,
                int zero_flag, int ct_ox, int ct_oy, int depth, int intra_cb, int final);
int venc_rec_intra_tb(f265_enc_thread *t, int comp, int lg_bs, int mode, int zero_flag, int ct_ox, int ct_oy,
                      int depth);
int venc_rec_intra_tt(f265_enc_thread *t, f265_cb *cb, int split_part_flag, int part_idx, int lg_bs,
                      int ct_ox, int ct_oy);
void venc_rec_inter_cb(f265_enc_thread *t, f265_cb *cb);
int venc_rec_inter_tt(f265_enc_thread *t, f265_cb *cb, f265_pix pred[3][64*64], int lg_bs, int cb_ox, int cb_oy);
void venc_rec_cb(f265_enc_thread *t, f265_cb *cb);
void venc_rec_ctb(f265_enc_thread *t);
int venc_do_rdoq(f265_rec_params *rp, int16_t *dst, int16_t *src);

#endif

